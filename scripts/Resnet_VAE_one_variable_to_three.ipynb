{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c85f5f0",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed8fd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init pack\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "psi = np.load('/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/data/actual/long_run_310k.npy')\n",
    "\n",
    "psi = psi[:,1,:]\n",
    "\n",
    "# Normalization\n",
    "mean_psi = np.mean(psi, axis=0, keepdims=True)\n",
    "std_psi = np.std(psi, axis=0, keepdims=True)\n",
    "psi = (psi - mean_psi) / std_psi\n",
    "\n",
    "# Pre-processing\n",
    "\n",
    "lead = 1\n",
    "trainN = 250000\n",
    "valN = 50000\n",
    "index = 63\n",
    "\n",
    "# Defining the variable ranges\n",
    "variable_range = [(0,24), (25, 49), (50, 74), (0, 49), (0,74)]\n",
    "\n",
    "# Select the variable: 0 for real perturbation, 1 for imaginary perturbation, 2 for zonal winds\n",
    "variable = 3\n",
    "num_variables = variable_range[variable][1] - variable_range[variable][0] + 1\n",
    "\n",
    "# Shuffle and map indices\n",
    "np.random.seed(42)\n",
    "valid_indices = np.arange(0, trainN - lead)\n",
    "shuffled_indices = np.random.permutation(valid_indices)\n",
    "\n",
    "# Now constrain the shuffled indices to the variable range\n",
    "np_psi_train_input = psi[shuffled_indices, variable_range[variable][0]:variable_range[variable][1]+1]\n",
    "np_psi_train_label = psi[shuffled_indices + lead, :]\n",
    "\n",
    "psi_train_input = torch.tensor(np_psi_train_input)\n",
    "psi_train_label = torch.tensor(np_psi_train_label)\n",
    "\n",
    "np_psi_val_input = psi[trainN:trainN+valN, variable_range[variable][0]:variable_range[variable][1]+1]\n",
    "np_psi_val_label = psi[trainN+lead:trainN+valN+lead, :]\n",
    "psi_val_input = torch.tensor(np_psi_val_input)\n",
    "psi_val_label =  torch.tensor(np_psi_val_label)\n",
    "\n",
    "plt.plot(np_psi_val_input[:,-1]) # Real and Imaginary PSI\n",
    "plt.plot(np_psi_val_label[:,-1]) # Real and Imaginary PSI + Zonal Wind\n",
    "plt.show()\n",
    "# plt.plot(psi_val_input[0:50000,63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd818a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder (MLP)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(75, 512)  # Input layer (2 + 2) -> Hidden layer (128)\n",
    "        self.fc2 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc3 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc4 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc5 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc6 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)  # Hidden layer (128) -> Latent space (2)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)  # Hidden layer (128) -> Log variance (2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Activation function for hidden layer\n",
    "        x = torch.relu(self.fc2(x)) + x\n",
    "        x = torch.relu(self.fc3(x)) + x\n",
    "        x = torch.relu(self.fc4(x)) + x\n",
    "        # x = torch.relu(self.fc5(x)) + x\n",
    "        # x = torch.relu(self.fc6(x)) + x\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the decoder (MLP)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, condition_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim + condition_dim, 512)  # Input layer (2 + 2) -> Hidden layer (128)\n",
    "        self.fc2 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc3 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc4 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc5 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc6 = nn.Linear(512, 512)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc_output = nn.Linear(512, output_dim)  # Hidden layer (128) -> Output layer (2)\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        z = torch.cat((z, condition), dim=1)  # Concatenate latent vector and condition\n",
    "        z = torch.relu(self.fc1(z))  # Activation function for hidden layer\n",
    "        z = torch.relu(self.fc2(z)) + z\n",
    "        z = torch.relu(self.fc3(z)) + z\n",
    "        z = torch.relu(self.fc4(z)) + z\n",
    "        # z = torch.relu(self.fc5(z)) + z\n",
    "        # z = torch.relu(self.fc6(z)) + z\n",
    "        output = self.fc_output(z)\n",
    "        return output\n",
    "\n",
    "# Define the VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, condition_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_dim, condition_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        return self.decoder(z, condition)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        output = self.decode(z, condition)\n",
    "        return output, mu, logvar\n",
    "\n",
    "output_dim = 75\n",
    "latent_dim = 32\n",
    "condition_dim = num_variables\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a446e18",
   "metadata": {},
   "source": [
    "# CHOOSING BY EXP FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386ce575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Check if crps is correct\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import warnings\n",
    "from torch.serialization import SourceChangeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def normalize_transition_time(s, dlt, r):\n",
    "    \"\"\"\n",
    "    Normalize the transition time based on the specified delta and reference value.\n",
    "\n",
    "    Args:\n",
    "        s (float): The transition time to normalize.\n",
    "        dlt (float): The delta value for normalization.\n",
    "        r (float): The reference value for normalization.\n",
    "\n",
    "    Returns:\n",
    "        norm (float): The normalized transition time.\n",
    "    \"\"\"\n",
    "    norm = 1 - np.exp(-np.abs((s - r)) / dlt)\n",
    "    return norm\n",
    "\n",
    "# Code from Ira Shokar but slightly changed\n",
    "def crps_score(p, y):\n",
    "    \"\"\"\n",
    "    Calculate CRPS for given predictions and observations.\n",
    "\n",
    "    Args:\n",
    "        p (Tensor): Predictions, shape (N, D) where N = ens_num and D is the dimension of the prediction.\n",
    "        y (Tensor): Observations, shape (D) where D is the dimension of the observation.\n",
    "\n",
    "    Returns:\n",
    "        crps (float): The CRPS score.\n",
    "    \"\"\"\n",
    "    y  = y.unsqueeze(0)\n",
    "    # First term: mean distance from observations to ensemble members\n",
    "    mae     = torch.cdist(y, p, 1).mean()\n",
    "    # Second term: mean distance between ensemble members (properly normalized)\n",
    "    ens_var = torch.cdist(p, p, 1).mean()\n",
    "    \n",
    "    return mae - 0.5 * ens_var\n",
    "\n",
    "# Function to calculate transition durations\n",
    "def calculate_transition_durations(y, u, l):\n",
    "    \"\"\"\n",
    "    Calculate the return periods with user-defined upper and lower bounds.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): The time series data.\n",
    "        u (float): The upper bound for the transition.\n",
    "        l (float): The lower bound for the transition.\n",
    "\n",
    "    Returns:\n",
    "        t_times (list): The list of return periods for transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    t_times = []\n",
    "    s = None\n",
    "    above_u = False\n",
    "    below_l = False\n",
    "    for i in range(1, len(y)):\n",
    "        if y[i] < l:  \n",
    "            below_l = True\n",
    "            above_u = False\n",
    "        elif y[i] > u:  \n",
    "            if below_l and s is not None:\n",
    "                t_times.append(i - s)\n",
    "                s = None  \n",
    "            above_u = True\n",
    "            below_l = False\n",
    "\n",
    "        if below_l and s is None:\n",
    "            s = i\n",
    "    return t_times\n",
    "\n",
    "def KL_coefficient(r, p, delta, cycle, KL_by_dim_cycle):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence between two distributions and normalize it.\n",
    "\n",
    "    Args:\n",
    "        r (np.array): Real distribution.\n",
    "        p (np.array): Predicted distribution.\n",
    "        delta (float): Delta value for normalization.\n",
    "        cycle (int): Cycle number for tracking.\n",
    "        KL_by_dim_cycle (dict): Dictionary to store KL divergence values by dimension and cycle.\n",
    "\n",
    "    Returns:\n",
    "        r (np.array): Processed real distribution.\n",
    "        p (np.array): Processed predicted distribution.\n",
    "        nkl (float): Normalized KL divergence.\n",
    "    \"\"\"\n",
    "    # Calculating KL divergence\n",
    "    r = r[:30000, 1, 63]\n",
    "    p = p[:30000]\n",
    "    \n",
    "    rh, b = np.histogram(r, bins=50, density=True)\n",
    "    ph, _ = np.histogram(p, bins=b, density=True)\n",
    "\n",
    "    e = 1e-10\n",
    "    rh += e\n",
    "    ph += e\n",
    "\n",
    "    # Calculate KL divergence between the two histograms\n",
    "    kl = np.sum(rh * np.log(rh / ph))\n",
    "    nkl = normalize_transition_time(kl, 1, 0)\n",
    "\n",
    "    print(f\"Normalized KL divergence for delta {delta}, cycle {cycle}: {nkl:.6f}\")\n",
    "    KL_by_dim_cycle[delta][cycle].append(nkl)\n",
    "\n",
    "    return r, p, nkl\n",
    "\n",
    "def CCDF_fit(p_times, s):\n",
    "    \"\"\"\n",
    "    Calculate the slope of the CCDF of transition times and normalize it.\n",
    "\n",
    "    Args:\n",
    "        p_times (list): Transition times from predictions.\n",
    "        s (float): Real value for normalization.\n",
    "    \n",
    "    Returns:\n",
    "        np_slope (float): Normalized slope of the CCDF.\n",
    "    \"\"\"\n",
    "    # === PREDICTIONS CCDF AND FIT ===\n",
    "    if len(p_times) > 0 and len(np.unique(p_times)) > 1:\n",
    "        sp_times = np.sort(p_times)\n",
    "        p_ccdf = 1 - np.arange(1, len(sp_times) + 1) / len(sp_times)\n",
    "\n",
    "        p_v_indices = p_ccdf > 0\n",
    "        px_fit = sp_times[p_v_indices]\n",
    "        py_fit = np.log(p_ccdf[p_v_indices])\n",
    "\n",
    "        p_slope, _, *_ = linregress(px_fit, py_fit)\n",
    "        np_slope = normalize_transition_time(p_slope, 0.005, s)\n",
    "        return np_slope\n",
    "\n",
    "    else:\n",
    "        print(\"No transitions detected in predictions for CCDF slope evaluation.\")\n",
    "\n",
    "def Mean_and_std_of_predictions(p_times, r_times, dlt, cc, transitions_by_dim_cycle, transitions_normalized_by_dim_cycle, transitions_normalized_std_by_dim_cycle):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of transition times from predictions and normalize them.\n",
    "\n",
    "    Args:\n",
    "        p_times (list): Transition times from predictions.\n",
    "        r_times (list): Transition times from real data.\n",
    "        dlt (float): Delta value for normalization.\n",
    "        cc (int): Cycle number for tracking.\n",
    "        transitions_by_dim_cycle (dict): Dictionary to store transition times by dimension and cycle.\n",
    "        transitions_normalized_by_dim_cycle (dict): Dictionary to store normalized transition times by dimension and cycle.\n",
    "        transitions_normalized_std_by_dim_cycle (dict): Dictionary to store normalized standard deviations by dimension and cycle.\n",
    "\n",
    "    Returns:\n",
    "        npd_mean (float): Normalized mean of transition times.\n",
    "        npd_std (float): Normalized standard deviation of transition times.\n",
    "    \"\"\"\n",
    "    p_mean = np.mean(p_times)\n",
    "    p_std = np.std(p_times)\n",
    "\n",
    "    pd_mean = abs(p_mean - np.mean(r_times))\n",
    "    pd_std = abs(p_std - np.std(r_times))\n",
    "\n",
    "    npd_mean = normalize_transition_time(pd_mean, 1000, np.mean(r_times))\n",
    "    npd_std = normalize_transition_time(pd_std, 1000, np.std(r_times))\n",
    "\n",
    "    npd_std = 1 if npd_std == 0 else npd_std\n",
    "\n",
    "    transitions_by_dim_cycle[dlt][cc].append(pd_mean)\n",
    "    transitions_normalized_by_dim_cycle[dlt][cc].append(npd_mean)\n",
    "    transitions_normalized_std_by_dim_cycle[dlt][cc].append(npd_std)\n",
    "\n",
    "    return npd_mean, npd_std\n",
    "\n",
    "# KL Annealing (FROM PAPER)\n",
    "def frange_cycle_linear(start, stop, n_epoch, n_cycle=4, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Generate a linear schedule for KL annealing over multiple cycles.\n",
    "\n",
    "    Args:\n",
    "        start (float): Starting value of the schedule.\n",
    "        stop (float): Stopping value of the schedule.\n",
    "        n_epoch (int): Total number of epochs.\n",
    "        n_cycle (int): Number of cycles for the schedule.\n",
    "        ratio (float): Ratio of the cycle length to the total number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        L (np.array): Array containing the linear schedule values for each epoch.\n",
    "    \"\"\"\n",
    "    L = np.ones(n_epoch)\n",
    "    period = n_epoch/n_cycle\n",
    "    step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "    for c in range(n_cycle):\n",
    "\n",
    "        v , i = start , 0\n",
    "        while v <=stop and (int(i+c*period) < n_epoch):\n",
    "            L[int(i+c*period)] = v\n",
    "            v += step\n",
    "            i += 1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def Timeseries_plot(y, p, ep, ax):\n",
    "    \"\"\"\n",
    "    Plot the timeseries.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): Actual zonal wind values.\n",
    "        p (np.array): Predicted zonal wind values.\n",
    "        ep (int): Current epoch number.\n",
    "        ax (matplotlib.axes.Axes): Axes object to plot on.\n",
    "\n",
    "    Returns:\n",
    "        None. Just plots the timeseries on the provided axes.\n",
    "    \"\"\"\n",
    "    ax.plot(y, 'b', label='Actual')\n",
    "    ax.plot(p, 'r', label='Predictions')\n",
    "\n",
    "    ax.set_title(f\"Timeseries | Epoch {ep}\", fontsize=16)\n",
    "    ax.set_xlabel('Time Step', fontsize=14)\n",
    "    ax.set_ylabel('Zonal Wind Value', fontsize=14)\n",
    "\n",
    "    ax.legend(['Predictions', 'Actual'])\n",
    "    ax.grid(True)\n",
    "\n",
    "    # save_path = os.path.join(folder, \"timeseries\")\n",
    "    # save_path = os.path.join(save_path, f\"timeseries_plot_{epoch+1}.png\")\n",
    "\n",
    "    # plt.savefig(save_path)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def PDF_plot(y, p, ep, pdf_dt, ax):\n",
    "    \"\"\"\n",
    "    Plot PDFs of the zonal wind values.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): Actual zonal wind values.\n",
    "        p (np.array): Predicted zonal wind values.\n",
    "        ep (int): Current epoch number.\n",
    "        pdf_dt (float): KL divergence between the PDFs of actual and predicted values.\n",
    "        ax (matplotlib.axes.Axes): Axes object to plot on.\n",
    "\n",
    "    Returns:\n",
    "        None. Just plots the PDFs on the provided axes.\n",
    "    \"\"\"\n",
    "\n",
    "    sns.histplot(y, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data', ax=ax)\n",
    "    sns.histplot(p, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions', ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Probability Distribution Functions (PDFs) | Epoch {ep} | KL Error: {pdf_dt:.4f}\", fontsize=16)\n",
    "    ax.set_xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    \n",
    "    ax.axvline(np.mean(y), color='black', linestyle='--', label=f'Real Mean: {np.mean(y):.2f}')\n",
    "    ax.axvline(np.mean(p), color='red', linestyle='--', label=f'Pred Mean: {np.mean(p):.2f}')\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # save_path = os.path.join(folder, \"bi_modal_distri\")\n",
    "    # save_path = os.path.join(save_path, f\"bi_modal_distribution_plot_{epoch+1}.png\")\n",
    "    # plt.savefig(save_path)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def Exp_fit_plot(xlp, yvp, xlr, yvr, p_exp_fit, r_exp_fit, ep, exp_d, range_d, ax):\n",
    "    \"\"\"\n",
    "    Plot the exponential fits of transition return periods.\n",
    "\n",
    "    Args:\n",
    "        xlp (np.array): X values for predicted exponential fit.\n",
    "        yvp (np.array): Y values for predicted exponential fit.\n",
    "        xlr (np.array): X values for real exponential fit.\n",
    "        yvr (np.array): Y values for real exponential fit.\n",
    "        p_exp_fit (float): Slope of the predicted exponential fit.\n",
    "        r_exp_fit (float): Slope of the real exponential fit.\n",
    "        ep (int): Current epoch number.\n",
    "        exp_d (float): Exponential fit error for predictions.\n",
    "        range_d (float): Range error for predictions.\n",
    "        ax (matplotlib.axes.Axes): Axes object to plot on.\n",
    "\n",
    "    Returns:\n",
    "        None. Just plots the exponential fits on the provided axes.\n",
    "    \"\"\"\n",
    "    ax.plot(xlp, yvp, 'r-', label=f'Pred Exp Fit (slope={p_exp_fit:.4f})', linewidth=2)\n",
    "    ax.plot(xlr, yvr, 'b-', label=f'Real Exp Fit (slope={r_exp_fit:.4f})', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Time Duration (Steps)')\n",
    "    ax.set_ylabel('Exponential Fit')\n",
    "    ax.set_title(f\"Exponential Fits of Transition Return Periods | Epoch {ep} | Exp Error: {exp_d:.4f} | Range Error: {range_d:.4f}\", fontsize=16)\n",
    "   \n",
    "    ax.set_yscale(\"linear\")  # y-axis log scale\n",
    "    ax.set_xscale(\"linear\")  # x-axis linear scale\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    \n",
    "    # save_path = os.path.join(folder, \"expo_fit\")\n",
    "    # save_path = os.path.join(save_path, f\"expo_fit_plot_{epoch}.png\")\n",
    "    # plt.savefig(save_path)\n",
    "    # plt.show()\n",
    "\n",
    "def Final_avg_transition_plot(transitions_by_dim_cycle, r, dlt, ncc, f):\n",
    "    \"\"\"\n",
    "    Plot the average transition values across all cycles.\n",
    "\n",
    "    Args:\n",
    "        transitions_by_dim_cycle (dict): Dictionary containing transition values by dimension and cycle.\n",
    "        r (float): Real data value for comparison.\n",
    "        dlt (float): Delta coefficient for normalization.\n",
    "        ncc (int): Number of cycles.\n",
    "        f (str): Folder path to save the plot.\n",
    "    \n",
    "    Returns:\n",
    "        None. Just plots the average transition values and saves the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for c in range(ncc):\n",
    "        plt.plot(transitions_by_dim_cycle[dlt][c], 'o-', label=f'Cycle {c}')\n",
    "    \n",
    "    plt.axhline(y=r, color='r', linestyle='--', label='Real Data')\n",
    "\n",
    "    plt.xlabel('Epoch within Cycle')\n",
    "    plt.ylabel('Average Transition Value')\n",
    "    plt.ylim(0.1,2000)\n",
    "    plt.title(f'Average Transition Progress (Delta Coefficient={dlt})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    f = os.path.join(f, f\"transition_plot_all_cycles.png\")\n",
    "    plt.savefig(f)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def Final_exp_fit_plot(exp_fit_by_dim_cycle, r_exp_fit, dlt, ncc, f):\n",
    "    \"\"\"\n",
    "    Plot the exponential fit values across all cycles.\n",
    "\n",
    "    Args:\n",
    "        exp_fit_by_dim_cycle (dict): Dictionary containing exponential fit values by dimension and cycle.\n",
    "        r_exp_fit (float): Real data value for comparison.\n",
    "        dlt (float): Delta coefficient for normalization.\n",
    "        ncc (int): Number of cycles.\n",
    "        f (str): Folder path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "        None. Just plots the exponential fit values and saves the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for c in range(ncc):\n",
    "        plt.plot(exp_fit_by_dim_cycle[dlt][c], 'o-', label=f'Cycle {c}')\n",
    "    \n",
    "    plt.axhline(y=r_exp_fit, color='r', linestyle='--', label='Real Data')\n",
    "\n",
    "    plt.xlabel('Epoch within Cycle')\n",
    "    plt.ylabel('Exponential Fit Value')\n",
    "    plt.title(f'Exponential Fit Progress (Delta Coefficient={dlt})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    f = os.path.join(f, f\"exponential_fit_plot_all_cycles.png\")\n",
    "    plt.savefig(f)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def all_plot(y, p, xlp, yvp, xlr, yvr, \n",
    "             p_exp_fit, r_exp_fit, pdf_dt, exp_dt, range_dt, ep, folder):\n",
    "    \"\"\"\n",
    "    Comprehensive plot with timeseries, PDF, and exponential fit.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): Actual zonal wind values.\n",
    "        p (np.array): Predicted zonal wind values.\n",
    "        xlp (np.array): X values for predicted exponential fit.\n",
    "        yvp (np.array): Y values for predicted exponential fit.\n",
    "        xlr (np.array): X values for real exponential fit.\n",
    "        yvr (np.array): Y values for real exponential fit.\n",
    "        p_exp_fit (float): Slope of the predicted exponential fit.\n",
    "        r_exp_fit (float): Slope of the real exponential fit.\n",
    "        pdf_dt (float): KL divergence between the PDFs of actual and predicted values.\n",
    "        exp_dt (float): Exponential fit error for predictions.\n",
    "        range_dt (float): Range error for predictions.\n",
    "        ep (int): Current epoch number.\n",
    "        folder (str): Folder path to save the plot.\n",
    "    \n",
    "    Returns:\n",
    "        None. Just plots the timeseries, PDF, and exponential fit and saves the figure.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig, width_ratios=[1,1], height_ratios=[1,1])\n",
    "\n",
    "    ax_timmeseries = fig.add_subplot(gs[:, 0])\n",
    "    ax_pdf = fig.add_subplot(gs[0, 1])\n",
    "    ax_exp_fit = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    Timeseries_plot(y, p, ep, ax_timmeseries)\n",
    "    PDF_plot(y, p, ep, pdf_dt, ax_pdf)\n",
    "    Exp_fit_plot(xlp, yvp, xlr, yvr, \n",
    "                 p_exp_fit, r_exp_fit, ep, exp_dt, range_dt, ax_exp_fit)\n",
    "    \n",
    "    dt = np.sqrt(pdf_dt**2 + exp_dt**2 + range_dt**2)\n",
    "    fig.suptitle(f\"Predictions vs Actual | Epoch {ep} | Euclidean Metric Error: {dt}\", fontsize=20)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.35)  # Adjust these values as desired\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    plt.savefig(os.path.join(folder, f\"plots/all_plots_epoch_{ep}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def Loss_plot(t_loss, v_loss, cc, dlt, f):\n",
    "    \"\"\"\n",
    "    Plot the training and validation losses.\n",
    "\n",
    "    Args:\n",
    "        t_loss (list): Training loss values.\n",
    "        v_loss (list): Validation loss values.\n",
    "        cc (int): Cycle number for tracking.\n",
    "        dlt (float): Delta value for normalization.\n",
    "        f (str): Folder path to save the plot.\n",
    "    \n",
    "    Returns:\n",
    "        None. Just plots the training and validation losses and saves the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    plt.plot(t_loss, label='Training Loss')\n",
    "    plt.plot(v_loss, label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Losses for Cycle {cc+1} with Delta {dlt}')\n",
    "    plt.legend()\n",
    "\n",
    "    f = os.path.join(f, f\"loss_plot_cycle_{cc+1}_delta_{dlt}.png\")\n",
    "    plt.savefig(f)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e54591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_kl_coef = frange_cycle_linear(0.01, 0.3, 3, 1, 1)\n",
    "print(beta_kl_coef)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Try to change KL metric to KS. Add all graphs in one figure and add the distance metric to the best model selection to it.\n",
    "\n",
    "# Training\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def model_restore(mp, model):\n",
    "    \"\"\"\n",
    "    Restore the model state from a saved checkpoint.\n",
    "\n",
    "    Args:\n",
    "        mp (str): Path to the model checkpoint.\n",
    "        model (nn.Module): The model to restore.\n",
    "\n",
    "    Returns:\n",
    "        None. The model state is loaded from the checkpoint if it exists.\n",
    "    \"\"\"\n",
    "    if os.path.exists(mp):\n",
    "        print(f\"Loading model from {mp}\")\n",
    "        model.load_state_dict(torch.load(mp))\n",
    "\n",
    "def inference(model, psi, tst, vr, v, nv, ld):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        psi (np.array): Input data for inference.\n",
    "        tst (int): Number of time steps.\n",
    "        vr (dict): Variable ranges for the input data.\n",
    "        v (int): Index of the variable to use for inference.\n",
    "        nv (int): Number of variables.\n",
    "        ld (int): Latent dimension of the model.\n",
    "    \n",
    "    Returns:\n",
    "        p (np.array): Predictions made by the model.\n",
    "    \"\"\"\n",
    "    s, e = vr[v][0], vr[v][1]+1\n",
    "    init_c = torch.reshape(torch.tensor(psi[0,s:e]), [1, nv])\n",
    "    z = torch.zeros([1,ld])\n",
    "    p = np.zeros ([tst, 75])\n",
    "\n",
    "    for k in range (0, tst):\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "\n",
    "                if (k ==0):\n",
    "\n",
    "                    z = torch.randn_like(z).float().cuda(non_blocking=True)\n",
    "                    init_c = init_c.float().cuda(non_blocking=True)\n",
    "\n",
    "                    y = (model.decode(z, init_c)).detach().cpu().numpy()\n",
    "                    p[k,:] = y\n",
    "\n",
    "                    init_c = torch.tensor(y[:, s:e])\n",
    "\n",
    "                else:\n",
    "                    z = torch.randn_like(z).float().cuda(non_blocking=True)\n",
    "                    init_c = torch.reshape(torch.tensor(p[k-1,s:e]),[1,nv]).float().cuda(non_blocking=True)\n",
    "\n",
    "                    y = (model.decode(z,init_c)).detach().cpu().numpy()\n",
    "                    p[k,:] = y\n",
    "\n",
    "                    init_c = torch.tensor(y[:, s:e])\n",
    "    \n",
    "    return p\n",
    "\n",
    "def euclidean_distance_for_predictions(ms):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance for a list of metrics.\n",
    "\n",
    "    Args:\n",
    "        ms (list): List of computed metrics.\n",
    "\n",
    "    Returns:\n",
    "        dt (float): The Euclidean distance calculated from the given computed metrics.\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    for m in ms:\n",
    "        s += m ** 2\n",
    "    dt = np.sqrt(s)\n",
    "    return dt\n",
    "\n",
    "def save_best_cycle_epoch(models, dlt, cc, ep, f,\n",
    "                          exp_fit_normalized_by_dim_cycle, \n",
    "                          KL_by_dim_cycle, duration_diff_by_dim_cycle, \n",
    "                          best_models_saved, best_models):\n",
    "    \"\"\"\n",
    "    Select the best model from a cycle based on combined distance metrics and save it.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of model paths for the current cycle.\n",
    "        dlt (float): Delta coefficient for normalization.\n",
    "        cc (int): Current cycle number.\n",
    "        ep (int): Current epoch number.\n",
    "        f (str): Folder path to save the best model.\n",
    "        exp_fit_normalized_by_dim_cycle (dict): Dictionary containing normalized exponential fit values by dimension and cycle.\n",
    "        KL_by_dim_cycle (dict): Dictionary containing KL divergence values by dimension and cycle.\n",
    "        duration_diff_by_dim_cycle (dict): Dictionary containing range differences by dimension and cycle.\n",
    "        best_models_saved (list): List to store the paths of the best models saved.\n",
    "        best_models (list): List to store the best models selected.\n",
    "\n",
    "    Returns:\n",
    "        None. The best model is saved to the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    b_model = None\n",
    "    b_i = -1\n",
    "    b_dt = float('inf')\n",
    "\n",
    "    for i in range(len(models)):  # models contains each epoch's model in the current cycle\n",
    "        print(exp_fit_normalized_by_dim_cycle[dlt][cc][i])\n",
    "        print(KL_by_dim_cycle[dlt][cc][i])\n",
    "        print(duration_diff_by_dim_cycle[dlt][cc][i])\n",
    "        ms = [exp_fit_normalized_by_dim_cycle[dlt][cc][i],\n",
    "                   KL_by_dim_cycle[dlt][cc][i], \n",
    "                   duration_diff_by_dim_cycle[dlt][cc][i]]\n",
    "        \n",
    "        dt = euclidean_distance_for_predictions(ms)\n",
    "        if dt < b_dt:\n",
    "            b_dt = dt\n",
    "            shutil.copyfile(models[i], f\"{f}/best_model_combined_distance_at_cycle_{cc}_and_checkpoint_{ep}.pth\")\n",
    "            print(f\"New best model saved with distance {dt:.4f} at epoch {i+1}\")\n",
    "            b_i = i\n",
    "            b_model = models[i]\n",
    "\n",
    "    if b_i != -1:\n",
    "        best_models_saved.append(b_model)\n",
    "        best_models.append((cc, b_i))\n",
    "\n",
    "def save_best_epoch(best_models, best_models_saved, exp_fit_normalized_by_dim_cycle,\n",
    "                    KL_by_dim_cycle, duration_diff_by_dim_cycle, dlt, mf):\n",
    "    \n",
    "    \"\"\"\n",
    "    Select the best model from the master training run based on combined distance metrics and save it.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of model paths for the current cycle.\n",
    "        dlt (float): Delta coefficient for normalization.\n",
    "        cc (int): Current cycle number.\n",
    "        ep (int): Current epoch number.\n",
    "        f (str): Folder path to save the best model.\n",
    "        exp_fit_normalized_by_dim_cycle (dict): Dictionary containing normalized exponential fit values by dimension and cycle.\n",
    "        KL_by_dim_cycle (dict): Dictionary containing KL divergence values by dimension and cycle.\n",
    "        duration_diff_by_dim_cycle (dict): Dictionary containing range differences by dimension and cycle.\n",
    "        best_models_saved (list): List to store the paths of the best models saved.\n",
    "        best_models (list): List to store the best models selected.\n",
    "\n",
    "    Returns:\n",
    "        None. The best model is saved to the specified folder.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Selecting the best model based on combined distance...\")\n",
    "\n",
    "    # Ensure best_models is not empty\n",
    "    if not best_models:\n",
    "        print(\"No best models found.\")\n",
    "        return\n",
    "    \n",
    "    # After all cycles - final best model selection\n",
    "    b_model = None\n",
    "    w_model = (-1, -1)\n",
    "    b_model_dt = float('inf')\n",
    "\n",
    "    print(f\"Number of best models saved: {len(best_models)}\")\n",
    "    for idx, (cc, ep_idx) in enumerate(best_models):\n",
    "\n",
    "        ms = [exp_fit_normalized_by_dim_cycle[dlt][cc][ep_idx], \n",
    "                   KL_by_dim_cycle[dlt][cc][ep_idx], \n",
    "                   duration_diff_by_dim_cycle[dlt][cc][ep_idx]]\n",
    "        \n",
    "        dt = euclidean_distance_for_predictions(ms)        \n",
    "        print(f\"Distance for model from cycle {cc+1}, epoch {ep_idx+1}: {dt:.4f}\")\n",
    "        print(f\"Current best distance: {b_model_dt:.4f}\")\n",
    "        \n",
    "        if dt < b_model_dt:\n",
    "            b_model_dt = dt\n",
    "            b_model = best_models_saved[idx]\n",
    "            w_model = (cc, ep_idx)\n",
    "\n",
    "    # Save the best model  \n",
    "    i,n = w_model\n",
    "    cc = i\n",
    "    ep = n\n",
    "\n",
    "    if cc == -1:\n",
    "        print(\"No best model found.\")\n",
    "    else:\n",
    "        shutil.copyfile(b_model, f\"{mf}/best_model_combined_distance_with_cycle_{cc+1}_and_epoch_{ep+1}.pth\")\n",
    "        print(f\"Best model saved with cycle {cc+1} and epoch {ep+1}.\")\n",
    "\n",
    "# Initialization\n",
    "scaler          = GradScaler()\n",
    "\n",
    "latent_dims     = [1024]\n",
    "latent_dim      = 32\n",
    "kl_coefficients = [0.1]\n",
    "kl_coef         = 0.1\n",
    "delta_coefs     = [1]\n",
    "time_step       = 30000\n",
    "num_cycles      = 1\n",
    "ens_size        = 1\n",
    "level           = 63\n",
    "upper_bound     = 53.8 / 2.8935\n",
    "lower_bound     = 7.41\n",
    "\n",
    "TRAIN_N         = 250000\n",
    "VAL_N           = 50000\n",
    "\n",
    "restore         = True\n",
    "\n",
    "best_distance   = float('inf')\n",
    "\n",
    "real_data       = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/data/actual/long_run_310k.npy\")\n",
    "real_data_1d    = real_data[:, 1, level]\n",
    "\n",
    "# Function to calculate transition durations\n",
    "real_durations      = calculate_transition_durations(real_data_1d, upper_bound, lower_bound)\n",
    "real_data_sorted    = np.sort(real_durations)\n",
    "transition_real     = np.mean(real_data_sorted)\n",
    "\n",
    "actual_hist, bin_edges = np.histogram(real_data[:, 1, level], bins=50, density=True)\n",
    "print(f\"Reference Real Data average_transition_time: {transition_real}\")\n",
    "\n",
    "# Compute CCDF slope for real data\n",
    "ccdf_real           = 1 - np.arange(1, len(real_data_sorted) + 1) / len(real_data_sorted)\n",
    "valid_indices_real  = ccdf_real > 0\n",
    "x_fit_real          = real_data_sorted[valid_indices_real]\n",
    "y_fit_real          = np.log(ccdf_real[valid_indices_real])\n",
    "slope_real, intercept_real, *_ = linregress(x_fit_real, y_fit_real)\n",
    "print(f\"Reference Real Data CCDF Slope: {slope_real}\")\n",
    "\n",
    "# Compute exponential fit for real data\n",
    "x_line_real = np.linspace(min(real_data_sorted), max(real_data_sorted), 100)\n",
    "exponential_fit_real = 1/np.mean(real_data_sorted)\n",
    "y_values_real = exponential_fit_real*x_line_real\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "transitions_by_dim_cycle                = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "transitions_normalized_by_dim_cycle     = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "transitions_normalized_std_by_dim_cycle = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "duration_diff_by_dim_cycle              = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "slope_diff_by_dim_cycle                 = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "exp_fit_by_dim_cycle                    = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "exp_fit_normalized_by_dim_cycle         = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "KL_by_dim_cycle                         = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "\n",
    "models_by_dim_cycle = []\n",
    "master_folder = f\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/Resnet_VAE_model_DELTA_TEST_at_{datetime.datetime.now()}\"\n",
    "os.makedirs(master_folder)\n",
    "\n",
    "for delta in delta_coefs:\n",
    "    print(f\"USING DELTA COEF OF {delta}\")\n",
    "\n",
    "    best_models         = []\n",
    "    best_models_saved   = []\n",
    "\n",
    "    losses_training     = []\n",
    "    losses_validation   = []\n",
    "\n",
    "    for cycle in range(0,num_cycles):\n",
    "\n",
    "        models = []\n",
    "\n",
    "        # Initialize the model, optimizer, and loss function\n",
    "        model       = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "        model       = model.cuda()\n",
    "        optimizer   = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        # Create the folder structure for saving results\n",
    "        subfolders  = ['plots','checkpoints']\n",
    "        folder      = f\"{master_folder}/model_at_{cycle}_with_delta_{delta}\"\n",
    "        os.makedirs(folder)\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(folder, subfolder)\n",
    "            os.mkdir(path)\n",
    "\n",
    "        # Restore the model if specified\n",
    "        if restore:\n",
    "            model_path = \"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/Resnet_VAE_model_DELTA_TEST_base_1000_epoch/model_at_0_with_delta_1/checkpoint_1000\"\n",
    "            model_restore(model_path, model)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in range (0, TRAIN_N, batch_size):\n",
    "                \n",
    "                input_batch = psi_train_input[batch:batch + batch_size,:]\n",
    "                label_batch = psi_train_label[batch:batch + batch_size,:]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs, mus, logvars = zip(*[model(label_batch.float().cuda(non_blocking=True), input_batch.float().cuda(non_blocking=True)) for _ in range(ens_size)])\n",
    "                    output = torch.stack(outputs)\n",
    "                    mu = torch.stack(mus)\n",
    "                    logvar = torch.stack(logvars)\n",
    "\n",
    "                    reconstruction_loss = F.smooth_l1_loss(output, label_batch.float().cuda(non_blocking=True), reduction=\"mean\")\n",
    "                    crps_loss = crps_score(output, label_batch.float().cuda(non_blocking=True)) * 0.0001\n",
    "                    kl_loss = 0.5 * (mu ** 2 + torch.exp(logvar) - 1 - logvar).sum() * beta_kl_coef[epoch]\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = reconstruction_loss + kl_loss + 0*crps_loss\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            losses_training.append(loss.item())\n",
    "            print(f'''Epoch {epoch+1}, \n",
    "                Reconstrunction Loss: {reconstruction_loss.item()}\n",
    "                KL Loss: {kl_loss.item()}\n",
    "                CRPS Loss: {crps_loss.item()}\n",
    "                Total Loss: {loss.item()}\n",
    "                ''')\n",
    "\n",
    "            # Validation Loss\n",
    "            for batch in range (0, VAL_N, batch_size):\n",
    "\n",
    "                model.eval()\n",
    "                input_batch = psi_val_input[batch:batch + batch_size,:]\n",
    "                label_batch = psi_val_label[batch:batch + batch_size,:]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        outputs, mus, logvars = zip(*[model(label_batch.float().cuda(non_blocking=True), input_batch.float().cuda(non_blocking=True)) for _ in range(ens_size)])\n",
    "                        output = torch.stack(outputs)\n",
    "                        mu = torch.stack(mus)\n",
    "                        logvar = torch.stack(logvars)\n",
    "\n",
    "                        val_reconstruction_loss = F.smooth_l1_loss(output, label_batch.float().cuda(non_blocking=True), reduction=\"mean\")\n",
    "                        crps_loss = crps_score(output, label_batch.float().cuda(non_blocking=True)) * 0.0001\n",
    "                        kl_loss = 0.5 * (mu ** 2 + torch.exp(logvar) - 1 - logvar).sum() * beta_kl_coef[epoch]  \n",
    "                \n",
    "                # Total loss\n",
    "                val_loss = val_reconstruction_loss + kl_loss + 0*crps_loss\n",
    "\n",
    "            losses_validation.append(val_loss.item())\n",
    "            print(f'''\n",
    "                Validation Reconstrunction Loss: {val_reconstruction_loss.item()}\n",
    "                Validation KL Loss: {kl_loss.item()}\n",
    "                Validation CRPS Loss: {crps_loss.item()}\n",
    "                Validation Total Loss: {val_loss.item()}''')\n",
    "            \n",
    "            # Inference\n",
    "            pred = inference(model, psi, time_step, variable_range, variable, \n",
    "                             num_variables, latent_dim)\n",
    "\n",
    "            # Denormalize final preds\n",
    "            pred_mean = pred[:time_step, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "            actual_values = psi[:time_step, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "            predictions_1d = pred_mean[:, 63]  # shape (300000,)\n",
    "\n",
    "            # # Plotting the timeseries\n",
    "            # Timeseries_plot(actual_values[:30000, 63], \n",
    "            #                 predictions_1d[:30000], \n",
    "            #                 epoch, folder)\n",
    "            \n",
    "            # Calculate transition durations for predictions\n",
    "            pred_durations = calculate_transition_durations(predictions_1d, \n",
    "                                                            upper_bound, \n",
    "                                                            lower_bound)\n",
    "\n",
    "            # Calculate KL coefficient\n",
    "            KL_real_data, KL_predictions, avg_norm_KL = KL_coefficient(real_data, \n",
    "                                                                           predictions_1d, \n",
    "                                                                           delta, \n",
    "                                                                           cycle, \n",
    "                                                                           KL_by_dim_cycle)\n",
    "            \n",
    "            # # Plotting the PDFs\n",
    "            # PDF_plot(KL_real_data, \n",
    "            #          KL_predictions, \n",
    "            #          epoch, folder)\n",
    "\n",
    "            # Calculate mean and standard deviation of predictions\n",
    "            transition_diff_normalized, transition_std_diff_normalized = Mean_and_std_of_predictions(pred_durations, \n",
    "                                                                                                     real_durations, \n",
    "                                                                                                     delta, cycle,\n",
    "                                                                                                     transitions_by_dim_cycle,\n",
    "                                                                                                     transitions_normalized_by_dim_cycle,\n",
    "                                                                                                     transitions_normalized_std_by_dim_cycle)\n",
    "\n",
    "            # Calculate the difference between exponential fits\n",
    "            slope_diff_normalized = 1\n",
    "            slope_diff_normalized = CCDF_fit(pred_durations, slope_real)\n",
    "            slope_diff_by_dim_cycle[delta][cycle].append(slope_diff_normalized)\n",
    "\n",
    "            # Initialize variables for exponential fit and range of transitions\n",
    "            exponential_fit_pred = 0\n",
    "            duration_diff_normalized = 1\n",
    "\n",
    "            if len(pred_durations) > 0 and  len(np.unique(pred_durations)) > 1:\n",
    "\n",
    "                # Calculate the exponential fit for predictions\n",
    "                x_line_pred = np.linspace(min(pred_durations), max(pred_durations), 100)\n",
    "                exponential_fit_pred = 1/np.mean(pred_durations)\n",
    "                exp_fit_by_dim_cycle[delta][cycle].append(exponential_fit_pred)\n",
    "                exp_fit_normalized = normalize_transition_time(exponential_fit_pred, 0.005, exponential_fit_real)\n",
    "                exp_fit_normalized_by_dim_cycle[delta][cycle].append(exp_fit_normalized)\n",
    "                print(f\"Exponential Fit Normalized: {exp_fit_normalized:.6f}\")\n",
    "\n",
    "                # Calculate the y-values for the exponential fit\n",
    "                y_values_pred = exponential_fit_pred*x_line_pred\n",
    "\n",
    "                # Plot the exponential fit for predictions\n",
    "                # Exp_fit_plot(x_line_pred, y_values_pred, x_line_real, y_values_real, exponential_fit_pred, exponential_fit_real, epoch+1, folder)\n",
    "\n",
    "                # Calculate the range of transitions\n",
    "                max_pred = np.max(pred_durations)\n",
    "                min_pred = np.min(pred_durations)\n",
    "                \n",
    "                if max_pred > 0:\n",
    "\n",
    "                    # Calculate the predicted range and normalize it\n",
    "                    difference = abs(max_pred - min_pred)\n",
    "                    duration_diff_normalized = normalize_transition_time(difference, 10000, abs(np.max(real_durations)-np.min(real_durations)))\n",
    "                    duration_diff_by_dim_cycle[delta][cycle].append(duration_diff_normalized)\n",
    "                    print(f\"Duration Difference Normalized: {duration_diff_normalized:.6f}\")\n",
    "                    all_plot(actual_values[:30000, 63],\n",
    "                                predictions_1d[:30000], \n",
    "                                x_line_pred, y_values_pred, \n",
    "                                x_line_real, y_values_real, \n",
    "                                exponential_fit_pred, exponential_fit_real, avg_norm_KL,\n",
    "                                exp_fit_normalized, duration_diff_normalized,\n",
    "                                epoch+1, folder)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"No distribution of transitions detected in predictions.\")\n",
    "            else:\n",
    "                exp_fit_by_dim_cycle[delta][cycle].append(exponential_fit_pred)\n",
    "                duration_diff_by_dim_cycle[delta][cycle].append(duration_diff_normalized)\n",
    "                exp_fit_normalized = normalize_transition_time(exponential_fit_pred, 0.005, exponential_fit_real)\n",
    "                exp_fit_normalized_by_dim_cycle[delta][cycle].append(exp_fit_normalized)\n",
    "                print(\"No transitions detected in predictions for exponential fit evaluation.\")\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate accuracy by euclidean distance with specified metrics\n",
    "            metrics = [exp_fit_normalized, avg_norm_KL, duration_diff_normalized]\n",
    "            distance = euclidean_distance_for_predictions(metrics)\n",
    "            print(f\"Epoch {epoch+1}: Exponential Transition Fit Predictions Normalized: {exp_fit_normalized}, KL Normalized = {avg_norm_KL}, Duration Difference Normalized = {duration_diff_normalized}, Combined Distance = {distance:.6f}\")\n",
    "            \n",
    "            # Save the model weights at each epoch\n",
    "            path = f\"{folder}/checkpoints/checkpoint_{epoch+1}\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f\"Model weights saved to {folder} with point {epoch+1}.\")\n",
    "            models.append(path)\n",
    "\n",
    "            # Final plots if last epoch\n",
    "            if epoch == num_epochs - 1:\n",
    "\n",
    "                folder = os.path.join(folder, \"summary\")\n",
    "                if not os.path.exists(folder):\n",
    "                    os.makedirs(folder)\n",
    "                # Plot the final average transition plot\n",
    "                Final_avg_transition_plot(transitions_by_dim_cycle, transition_real, delta, num_cycles, folder)\n",
    "                \n",
    "                # Plot the final exponential fit plot\n",
    "                Final_exp_fit_plot(exp_fit_by_dim_cycle, exponential_fit_real, delta, num_cycles, folder)\n",
    "        \n",
    "        # Save the best model for the current cycle\n",
    "        save_best_cycle_epoch(models, delta, cycle, epoch, folder, exp_fit_normalized_by_dim_cycle, \n",
    "                              KL_by_dim_cycle, duration_diff_by_dim_cycle,\n",
    "                              best_models_saved, best_models)\n",
    "\n",
    "        Loss_plot(losses_training, losses_validation, cycle, delta, folder)\n",
    "# Save the best model after all cycles\n",
    "save_best_epoch(best_models, best_models_saved, exp_fit_normalized_by_dim_cycle,\n",
    "                KL_by_dim_cycle, duration_diff_by_dim_cycle, delta, master_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea14b56",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713280b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Inference\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "latent_dim = 32\n",
    "output_dim = 75\n",
    "condition_dim = num_variables\n",
    "model = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "start, end = variable_range[variable][0], variable_range[variable][1]+1\n",
    "\n",
    "# MODIFY THIS LINE FOR MODEL TESTING\n",
    "past_model = True  # Set to True if you want to load past model weights\n",
    "if past_model:\n",
    "    model_weights_path = \"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/best_models_kinda/June23th.pth\"\n",
    "    if os.path.exists(model_weights_path):\n",
    "        model.load_state_dict(torch.load(model_weights_path))\n",
    "        print(f\"Model weights loaded from {model_weights_path}.\")\n",
    "\n",
    "for _ in range (0,1):\n",
    "\n",
    "    start, end = variable_range[variable][0], variable_range[variable][1]+1\n",
    "    initial_cond = torch.reshape(torch.tensor(psi[0,start:end]), [1, num_variables])\n",
    "    print(initial_cond.shape)\n",
    "    time_step = 300000\n",
    "    z = torch.zeros([1,latent_dim])\n",
    "    num_ens = 1\n",
    "    pred = np.zeros ([time_step, 75, num_ens])\n",
    "\n",
    "    for k in range (0, time_step):\n",
    "\n",
    "        for ens in range (0, num_ens):\n",
    "            if (k ==0):\n",
    "                z = torch.randn_like(z)\n",
    "                print(z.shape, initial_cond.shape)\n",
    "                y = (model.decode(z.float().cuda(non_blocking=True),initial_cond.float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "                pred[k,:,ens] = y\n",
    "                y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "\n",
    "            else:\n",
    "                select_ens = np.random.randint(0,num_ens,1)\n",
    "                z = torch.randn_like(z)\n",
    "                y = (model.decode(z.float().cuda(non_blocking=True),torch.reshape(torch.tensor(pred[k-1,start:end,select_ens]),[1,num_variables]).float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "                pred[k,:, ens] = y\n",
    "                y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "\n",
    "    # Denormalize final preds\n",
    "    pred = pred.reshape(pred.shape[0], pred.shape[1])\n",
    "    print(pred.shape, psi.shape)\n",
    "    pred_mean = pred[:300000, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "    \n",
    "    # Denormalize test labels\n",
    "    actual_values = psi[:300000, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "    print(actual_values)\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(pred_mean[0:30000, 63],'r')\n",
    "    plt.plot(actual_values[0:30000, 63],'b')\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Predictions vs Actual\")\n",
    "    plt.savefig(f'/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/prediction_vs_actual_{datetime.datetime.now()}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # MODIFY THIS LINE FOR MODEL TESTING\n",
    "    np.save(f'/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/predictions_best_checkpoint_and_cycle_Resnet_VAE_8_temp.npy', pred_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd62a0c",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94752682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAGS to determine testing\n",
    "plot_data = 1\n",
    "#what level do you want to plot\n",
    "level = 63\n",
    "CCDF = 1\n",
    "Bi_modal_distribution = 1\n",
    "single_step_profiles = 1\n",
    "#for the single_step_profiles\n",
    "NUM_SAMPLES = 5\n",
    "#what weights do you want to use?\n",
    "MODEL_PATH = r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/best_models_kinda/June23th.pth\"\n",
    "LEVEL = 63\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Load the data; shape = (300000, 2, 75)\n",
    "real_data = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/data/actual/long_run_310k.npy\")\n",
    "predictions = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/predictions_best_checkpoint_and_cycle_Resnet_VAE_8_temp.npy\")\n",
    "\n",
    "#reshape the predictions so that it matches the real_data shape\n",
    "print(predictions.shape)\n",
    "print(real_data.shape)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "folder = f\"testing_at_{timestamp}\"\n",
    "os.mkdir(folder)\n",
    "subfolders = ['timeseries', 'CCDF', 'bi_modal_distribution', 'single_step_profiles']\n",
    "# Create each subdirectory inside the main folder\n",
    "for subfolder in subfolders:\n",
    "    path = os.path.join(folder, subfolder)\n",
    "    os.mkdir(path)\n",
    "    print(f\"Created subfolder: {path}\")\n",
    "SAVE_DIR = os.path.join(folder, \"single_step_profiles\")\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "model = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# MODIFY THIS LINE FOR MODEL TESTING\n",
    "model_weights_path = \"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/Resnet_VAE_model_lat1024_at_2025-04-11 12:46:39.526973/model_at_0/checkpoint_3\"\n",
    "\n",
    "if os.path.exists(model_weights_path):\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    print(f\"Model weights loaded from {model_weights_path}.\")\n",
    "    \n",
    "if (plot_data):\n",
    "    #note that the value 300000 will have to change depending on the real and predictions data length\n",
    "    u_profile_real = real_data[:300000, 1, level]  # Match time length with predictions\n",
    "    u_profile_pred = predictions[:, level]\n",
    "    time_steps = np.arange(len(u_profile_pred))\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.plot(time_steps, u_profile_real, label='Real Data', alpha=0.7)\n",
    "    plt.plot(time_steps, u_profile_pred, label='Predictions', linestyle='--')\n",
    "\n",
    "\n",
    "    # Labels, legend, and formatting\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('U (m/s)')\n",
    "    plt.title(f'Time Series of U at Vertical Level {level}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"timeseries\")\n",
    "    save_path = os.path.join(save_path, \"real_prediction_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "if (CCDF):\n",
    "    real_data_1d = real_data[:, 1, 63]  # Now shape is (309700,)\n",
    "    predictions_1d = predictions[:, 63]  # shape (300000,)\n",
    "\n",
    "    # Define bounds (assuming they apply to both datasets)\n",
    "    upper_bound = 53.8 / 2.8935\n",
    "    lower_bound = 7.41\n",
    "\n",
    "    # Function to calculate transition durations\n",
    "    def calculate_transition_durations(y_values, upper_bound, lower_bound):\n",
    "        times_between_transitions = []\n",
    "        transition_start = None\n",
    "        above_upper = False\n",
    "        below_lower = False\n",
    "\n",
    "        for i in range(1, len(y_values)):\n",
    "            if y_values[i] < lower_bound:  \n",
    "                below_lower = True\n",
    "                above_upper = False\n",
    "            elif y_values[i] > upper_bound:  \n",
    "                if below_lower and transition_start is not None:\n",
    "                    times_between_transitions.append(i - transition_start)\n",
    "                    transition_start = None  \n",
    "                above_upper = True\n",
    "                below_lower = False\n",
    "\n",
    "            if below_lower and transition_start is None:\n",
    "                transition_start = i\n",
    "\n",
    "        return times_between_transitions\n",
    "\n",
    "    # Compute transition durations for real data\n",
    "    real_durations = calculate_transition_durations(real_data_1d, upper_bound, lower_bound)\n",
    "\n",
    "    # Compute transition durations for predictions data\n",
    "    pred_durations = calculate_transition_durations(predictions_1d, upper_bound, lower_bound)\n",
    "\n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # === REAL DATA CCDF AND FIT ===\n",
    "    if len(real_durations) == 0:\n",
    "        print(\"No transitions detected in real data with current bounds!\")\n",
    "    else:\n",
    "        real_data_sorted = np.sort(real_durations)\n",
    "        x_line_real = np.linspace(min(real_data_sorted), max(real_data_sorted), 100)\n",
    "        exponential_fit_real = 1/np.mean(real_data_sorted)\n",
    "        y_values_real = exponential_fit_real*x_line_real\n",
    "        plt.plot(x_line_real, y_values_real, 'b-', label=f'Real Exp Fit (slope={exponential_fit_real:.4f})', linewidth=2)\n",
    "\n",
    "    # === PREDICTIONS CCDF AND FIT ===\n",
    "    if len(pred_durations) == 0:\n",
    "        print(\"No transitions detected in predictions with current bounds!\")\n",
    "    else:\n",
    "        pred_data_sorted = np.sort(pred_durations)\n",
    "        x_line_pred = np.linspace(min(pred_data_sorted), max(pred_data_sorted), 100)\n",
    "        exponential_fit_pred = 1/np.mean(pred_data_sorted)\n",
    "        y_values_pred = exponential_fit_pred*x_line_pred\n",
    "        plt.plot(x_line_pred, y_values_pred, 'r-', label=f'Pred Exp Fit (slope={exponential_fit_pred:.4f})', linewidth=2)\n",
    "\n",
    "    print(1/np.mean(real_data_sorted))\n",
    "    print(1/np.mean(pred_data_sorted))\n",
    "    # Plot labels and formatting\n",
    "    plt.xlabel('Time Duration (Steps)')\n",
    "    plt.ylabel('CCDF')\n",
    "    plt.title('CCDF of Time Between B->A and A->B Transitions (Exponential Fit)')\n",
    "    plt.yscale(\"linear\")  # y-axis log scale\n",
    "    plt.xscale(\"linear\")  # x-axis linear scale\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"CCDF\")\n",
    "    save_path = os.path.join(save_path, \"CCDF_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "if (Bi_modal_distribution):\n",
    "    zonal_wind_data_real = real_data[:, 1, 63]  # variable index 1 (e.g., zonal wind), level 60\n",
    "    zonal_wind_data_predictions = predictions[:, 63]  # variable index 0 (predictions), level 60\n",
    "\n",
    "    print(f\"Shape of zonal_wind_data_real: {zonal_wind_data_real.shape}\")\n",
    "    print(f\"Shape of zonal_wind_data_predictions: {zonal_wind_data_predictions.shape}\")\n",
    "\n",
    "    # Plot the bimodal histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create histograms (overlaid)\n",
    "    sns.histplot(zonal_wind_data_real, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data')\n",
    "    sns.histplot(zonal_wind_data_predictions, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions')\n",
    "\n",
    "    # Customize plot labels and title\n",
    "    plt.title('Distribution of Zonal Winds For Real Data and Predictions', fontsize=16)\n",
    "    plt.xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "    # Add vertical lines at means\n",
    "    plt.axvline(np.mean(zonal_wind_data_real), color='black', linestyle='--', label=f'Real Mean: {np.mean(zonal_wind_data_real):.2f}')\n",
    "    plt.axvline(np.mean(zonal_wind_data_predictions), color='red', linestyle='--', label=f'Pred Mean: {np.mean(zonal_wind_data_predictions):.2f}')\n",
    "\n",
    "    # Final plot settings\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"bi_modal_distribution\")\n",
    "    save_path = os.path.join(save_path, \"bi_modal_distribution_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "if (single_step_profiles):\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # === Load model weights ===\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    # === Randomly sample time points from real data ===\n",
    "    time_indices = random.sample(range(0, real_data.shape[0] - 2), NUM_SAMPLES)\n",
    "    print(f\"Randomly sampled time steps: {time_indices}\")\n",
    "\n",
    "    # === Time series visualization ===\n",
    "    real_data_timeseries = real_data[:, 1, LEVEL]\n",
    "    time_steps_all = np.arange(len(real_data_timeseries))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_steps_all, real_data_timeseries, label=\"Real Data at Level 61\", color='blue')\n",
    "\n",
    "    # Mark sample points\n",
    "    for idx_num, idx in enumerate(time_indices):\n",
    "        plt.axvline(x=idx, color='green', linestyle='--', linewidth=2)\n",
    "    if len(time_indices) > 0:\n",
    "        plt.axvline(x=time_indices[0], color='green', linestyle='--', linewidth=2, label='Sampled Points')\n",
    "\n",
    "    plt.title(\"Real Data Time Series with Sampled Points Highlighted\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"U (m/s) at Level 61\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(SAVE_DIR, \"real_data_timeseries_with_samples.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "    # === Iterate over each sampled time point ===\n",
    "    for i, time_step in enumerate(time_indices):\n",
    "        next_time_step = time_step + 1\n",
    "\n",
    "        # === Real data: current and next ===\n",
    "        real_current = real_data[time_step, 1, :]       \n",
    "        real_next = real_data[next_time_step, 1, :]      \n",
    "\n",
    "        # === Normalize real_current and make prediction for next step ===\n",
    "        initial_cond = torch.reshape(torch.tensor(psi[time_step,start:end]), [1, num_variables])\n",
    "        z = torch.zeros([1,latent_dim])\n",
    "        num_ens = 1\n",
    "        pred = np.zeros ([time_step, 75, num_ens])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn_like(z)\n",
    "            print(z.shape, initial_cond.shape)\n",
    "            y = (model.decode(z.float().cuda(non_blocking=True),initial_cond.float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "\n",
    "        # === Denormalize predicted next ===\n",
    "        pred_next_denorm = y.squeeze() * std_psi.squeeze() + mean_psi.squeeze()\n",
    "\n",
    "        # === Extract U, Re(Psi), Im(Psi) components ===\n",
    "        # U profiles\n",
    "        U_current_real = real_current[51:74]\n",
    "        U_next_real = real_next[51:74]\n",
    "        U_next_pred = pred_next_denorm[51:74]\n",
    "\n",
    "        # Re(Psi) profiles\n",
    "        RePsi_current_real = real_current[0:24]\n",
    "        RePsi_next_real = real_next[0:24]\n",
    "        RePsi_next_pred = pred_next_denorm[0:24]\n",
    "\n",
    "        # Im(Psi) profiles\n",
    "        ImPsi_current_real = real_current[25:50]\n",
    "        ImPsi_next_real = real_next[25:50]\n",
    "        ImPsi_next_pred = pred_next_denorm[25:50]\n",
    "\n",
    "        # === Differences ===\n",
    "        U_diff_real = U_next_real - U_current_real\n",
    "        U_diff_pred = U_next_pred - U_current_real\n",
    "\n",
    "        RePsi_diff_real = RePsi_next_real - RePsi_current_real\n",
    "        RePsi_diff_pred = RePsi_next_pred - RePsi_current_real\n",
    "\n",
    "        ImPsi_diff_real = ImPsi_next_real - ImPsi_current_real\n",
    "        ImPsi_diff_pred = ImPsi_next_pred - ImPsi_current_real\n",
    "\n",
    "        # === Create a single figure with 3 rows (U, Re(Psi), Im(Psi)) ===\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # 3 rows, 2 columns (Profile and Difference)\n",
    "\n",
    "        z_levels_U = np.linspace(0, 70, 23)\n",
    "        z_levels_RePsi = np.linspace(0, 70, 24)\n",
    "        z_levels_ImPsi = np.linspace(0, 70, 25)\n",
    "\n",
    "        # --- U ---\n",
    "        axes[0, 0].plot(U_current_real, z_levels_U, 'x-', label=\"Real Current\")\n",
    "        axes[0, 0].plot(U_next_real, z_levels_U, 'd-', label=\"Real Next\")\n",
    "        axes[0, 0].plot(U_next_pred, z_levels_U, 's--', label=\"Predicted Next\")\n",
    "        axes[0, 0].set_title(f\"U Profiles @ Step {time_step}\")\n",
    "        axes[0, 0].set_xlabel(\"U (m/s)\")\n",
    "        axes[0, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        axes[0, 1].plot(U_diff_real, z_levels_U, 'xb', label=\"Real  (Next - Current)\")\n",
    "        axes[0, 1].plot(U_diff_pred, z_levels_U, 'o--r', label=\"Pred  (Next - Current)\")\n",
    "        axes[0, 1].set_title(\"U Difference (Next - Current)\")\n",
    "        axes[0, 1].set_xlabel(\"U (m/s)\")\n",
    "        axes[0, 1].legend()\n",
    "\n",
    "        # --- Re(Psi) ---\n",
    "        axes[1, 0].plot(RePsi_current_real, z_levels_RePsi, 'x-', label=\"Real Current\")\n",
    "        axes[1, 0].plot(RePsi_next_real, z_levels_RePsi, 'd-', label=\"Real Next\")\n",
    "        axes[1, 0].plot(RePsi_next_pred, z_levels_RePsi, 's--', label=\"Predicted Next\")\n",
    "        axes[1, 0].set_title(f\"Re(Psi) Profiles @ Step {time_step}\")\n",
    "        axes[1, 0].set_xlabel(\"Re(Psi)\")\n",
    "        axes[1, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[1, 0].legend()\n",
    "\n",
    "        axes[1, 1].plot(RePsi_diff_real, z_levels_RePsi, 'xb', label=\"Real  (Next - Current)\")\n",
    "        axes[1, 1].plot(RePsi_diff_pred, z_levels_RePsi, 'o--r', label=\"Pred  (Next - Current)\")\n",
    "        axes[1, 1].set_title(\"Re(Psi) Difference (Next - Current)\")\n",
    "        axes[1, 1].set_xlabel(\"Re(Psi)\")\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        # --- Im(Psi) ---\n",
    "        axes[2, 0].plot(ImPsi_current_real, z_levels_ImPsi, 'x-', label=\"Real Current\")\n",
    "        axes[2, 0].plot(ImPsi_next_real, z_levels_ImPsi, 'd-', label=\"Real Next\")\n",
    "        axes[2, 0].plot(ImPsi_next_pred, z_levels_ImPsi, 's--', label=\"Predicted Next\")\n",
    "        axes[2, 0].set_title(f\"Im(Psi) Profiles @ Step {time_step}\")\n",
    "        axes[2, 0].set_xlabel(\"Im(Psi)\")\n",
    "        axes[2, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[2, 0].legend()\n",
    "\n",
    "        axes[2, 1].plot(ImPsi_diff_real, z_levels_ImPsi, 'xb', label=\"Real  (Next - Current)\")\n",
    "        axes[2, 1].plot(ImPsi_diff_pred, z_levels_ImPsi, 'o--r', label=\"Pred  (Next - Current)\")\n",
    "        axes[2, 1].set_title(\"Im(Psi) Difference (Next - Current)\")\n",
    "        axes[2, 1].set_xlabel(\"Im(Psi)\")\n",
    "        axes[2, 1].legend()\n",
    "\n",
    "        # === Finalize and Save ===\n",
    "        plt.suptitle(f\"Single Step Profile Comparisons at Time Step {time_step}\", fontsize=18)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, f\"Profile_Summary_point_{time_step}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Combined plot saved for sampled point {time_step}\")\n",
    "\n",
    "    # Final debug\n",
    "    print(\"Finished processing all sampled points.\")\n",
    "        # Debugging prints\n",
    "    print(predictions.shape) \n",
    "    print(real_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45e5f2",
   "metadata": {},
   "source": [
    "# WHERE ARE WE DOING THE WORST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "predictions_1d = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/predictions_best_checkpoint_and_cycle_Resnet_VAE_1.npy\")\n",
    "real_data_1d = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/data/actual/long_run_310k.npy\")\n",
    "print(predictions_1d.shape, real_data_1d.shape)\n",
    "\n",
    "predictions_1d = predictions_1d.reshape(predictions_1d.shape[0], 1, predictions_1d.shape[1])  # Reshape to (300000, 1, 75)\n",
    "\n",
    "# Flatten the data to 1D arrays\n",
    "predictions_1d = predictions_1d[:, 0, :]  # Extracting the 63rd variable (e.g., zonal wind)\n",
    "real_data_1d = real_data_1d[:, 1, :]  # Extracting the 63rd variable (e.g., zonal wind)\n",
    "print(predictions_1d.shape, real_data_1d.shape)\n",
    "\n",
    "differences_in_tvd = []\n",
    "differences_in_ovl = []\n",
    "\n",
    "for feature in range(predictions_1d.shape[1]):\n",
    "    actual_hist, bin_edges = np.histogram(real_data_1d[:, feature], bins=50, density=True)\n",
    "    pred_hist, _ = np.histogram(predictions_1d[:, feature], bins=bin_edges, density=True)\n",
    "    tvd = total_variation_distance(pred_hist, actual_hist)\n",
    "    differences_in_tvd.append((tvd, feature))\n",
    "\n",
    "    # Calculate overlap coefficient (OVL) between the two histograms\n",
    "    ovl = np.sum(np.maximum(actual_hist,pred_hist)-np.minimum(actual_hist, pred_hist))\n",
    "    differences_in_ovl.append((ovl, feature))\n",
    "\n",
    "# Sort the differences in TVD\n",
    "sorted_differences_in_tvd = sorted(differences_in_tvd, key=lambda x: x[0])[::-1]\n",
    "sorted_differences_in_ovl = sorted(differences_in_ovl, key=lambda x: x[0])[::-1]\n",
    "\n",
    "# Print the sorted differences\n",
    "print(f\"Sorted Differences in TVD:{sorted_differences_in_tvd}\")\n",
    "print(f\"Sorted Differences in Overlap:{sorted_differences_in_ovl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b50532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "folder = f\"testing_at_{timestamp}\"\n",
    "\n",
    "for i in range(0,30):\n",
    "    print(f\"Row {sorted_differences_in_ovl[i][1]}: TVD = {sorted_differences_in_ovl[i][0]:.6f}\")\n",
    "    data_real = real_data_1d[:, sorted_differences_in_ovl[i][1]]  # variable index 1 (e.g., zonal wind), level _\n",
    "    data_predictions = predictions_1d[:, sorted_differences_in_ovl[i][1]]  # variable index 0 (predictions), level _\n",
    "\n",
    "    print(f\"Shape of zonal_wind_data_real: {data_real.shape}\")\n",
    "    print(f\"Shape of zonal_wind_data_predictions: {data_predictions.shape}\")\n",
    "\n",
    "    # Plot the bimodal histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create histograms (overlaid)\n",
    "    sns.histplot(data_real, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data')\n",
    "    sns.histplot(data_predictions, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions')\n",
    "\n",
    "    # Customize plot labels and title\n",
    "    plt.title(f\"Distribution of Feature {sorted_differences_in_ovl[i][1]} For Real Data and Predictions\", fontsize=16)\n",
    "    plt.xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "    # Add vertical lines at means\n",
    "    plt.axvline(np.mean(data_real), color='black', linestyle='--', label=f'Real Mean: {np.mean(data_real):.2f}')\n",
    "    plt.axvline(np.mean(data_predictions), color='red', linestyle='--', label=f'Pred Mean: {np.mean(data_predictions):.2f}')\n",
    "\n",
    "    # Final plot settings\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"bi_modal_distribution\")\n",
    "    save_path = os.path.join(save_path, \"bi_modal_distribution_plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b141d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aff295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
