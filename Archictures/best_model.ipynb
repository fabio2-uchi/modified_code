{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c85f5f0",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed8fd5bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Init pack\n",
    "\n",
    "#OS Libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.serialization import SourceChangeWarning\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "#Functionality Libraries\n",
    "from tqdm import tqdm\n",
    "from scipy.integrate import solve_ivp\n",
    "import random\n",
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import wandb\n",
    "import datetime\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "psi = np.load(r'/home/fabio/work/HM_and_AI_models/VAE_Model/data/long_run_310k.npy')\n",
    "psi = psi[:,1,:]\n",
    "mean_psi = np.mean(psi, axis=0, keepdims=True)\n",
    "std_psi = np.std(psi, axis=0, keepdims=True)\n",
    "psi = (psi - mean_psi) / std_psi\n",
    "\n",
    "\n",
    "# Pre-processing\n",
    "\n",
    "lead = 1\n",
    "trainN = 250000\n",
    "valN = 50000\n",
    "index = 63\n",
    "\n",
    "# Defining the variable ranges\n",
    "variable_range = [(0,24), (25, 49), (50, 74), (0, 49), (0,74)]\n",
    "\n",
    "# Select the variable: 0 for real perturbation, 1 for imaginary perturbation, 2 for zonal winds\n",
    "variable = 3\n",
    "num_variables = variable_range[variable][1] - variable_range[variable][0] + 1\n",
    "\n",
    "# Shuffle and map indices\n",
    "np.random.seed(42)\n",
    "valid_indices = np.arange(0, trainN - lead)\n",
    "shuffled_indices = np.random.permutation(valid_indices)\n",
    "\n",
    "\n",
    "# Now constrain the shuffled indices to the variable range\n",
    "# \n",
    "np_psi_train_input = psi[shuffled_indices, variable_range[variable][0]:variable_range[variable][1]+1]\n",
    "np_psi_train_label = psi[shuffled_indices + lead, :]\n",
    "\n",
    "psi_train_input = torch.tensor(np_psi_train_input)\n",
    "psi_train_label = torch.tensor(np_psi_train_label)\n",
    "\n",
    "np_psi_val_input = psi[trainN:trainN+valN, variable_range[variable][0]:variable_range[variable][1]+1]\n",
    "np_psi_val_label = psi[trainN+lead:trainN+valN+lead, :]\n",
    "psi_val_input = torch.tensor(np_psi_val_input)\n",
    "psi_val_label =  torch.tensor(np_psi_val_label)\n",
    "\n",
    "plt.plot(np_psi_val_input[:,-1]) # Real and Imaginary PSI\n",
    "plt.plot(np_psi_val_label[:,-1]) # Real and Imaginary PSI + Zonal Wind\n",
    "plt.show()\n",
    "\n",
    "#plt.plot(psi_val_input[0:50000,63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd818a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder (MLP)\n",
    "num_neurons = 128\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(75, num_neurons)  # Input layer (2 + 2) -> Hidden layer (128)\n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc4 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc5 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc6 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc_mu = nn.Linear(num_neurons, latent_dim)  # Hidden layer (128) -> Latent space (2)\n",
    "        self.fc_logvar = nn.Linear(num_neurons, latent_dim)  # Hidden layer (128) -> Log variance (2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Activation function for hidden layer\n",
    "        x = torch.relu(self.fc2(x)) + x\n",
    "        x = torch.relu(self.fc3(x)) + x\n",
    "        x = torch.relu(self.fc4(x)) + x\n",
    "        x = torch.relu(self.fc5(x)) + x\n",
    "        x = torch.relu(self.fc6(x)) + x\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the decoder (MLP)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, condition_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim + condition_dim, num_neurons)  # Input layer (2 + 2) -> Hidden layer (128)\n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc4 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc5 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc6 = nn.Linear(num_neurons, num_neurons)  # Hidden layer (128) -> Hidden layer (128)\n",
    "        self.fc_output = nn.Linear(num_neurons, output_dim)  # Hidden layer (128) -> Output layer (2)\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        z = torch.cat((z, condition), dim=1)  # Concatenate latent vector and condition\n",
    "        z = torch.relu(self.fc1(z))  # Activation function for hidden layer\n",
    "        z = torch.relu(self.fc2(z)) + z\n",
    "        z = torch.relu(self.fc3(z)) + z\n",
    "        z = torch.relu(self.fc4(z)) + z\n",
    "        z = torch.relu(self.fc5(z)) + z\n",
    "        z = torch.relu(self.fc6(z)) + z\n",
    "        output = self.fc_output(z)\n",
    "        return output\n",
    "\n",
    "# Define the VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, condition_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_dim, condition_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        return self.decoder(z, condition)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        output = self.decode(z, condition)\n",
    "        return output, mu, logvar\n",
    "\n",
    "output_dim = 75\n",
    "condition_dim = num_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a446e18",
   "metadata": {},
   "source": [
    "# CHOOSING BY EXP FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2889b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ce575",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def normalize_transition_time(slope_value, delta, transition_real):\n",
    "    normalized = 1 - np.exp(-np.abs((slope_value - transition_real)) / delta)\n",
    "    return normalized\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "# Code from Ira Shokar but slightly changed\n",
    "def crps_score(predictions, actual):\n",
    "    actual  = actual.unsqueeze(0)\n",
    "    # First term: mean distance from observations to ensemble members\n",
    "    mae     = torch.cdist(actual, predictions, 1).mean()\n",
    "    # Second term: mean distance between ensemble members (properly normalized)\n",
    "    ens_var = torch.cdist(predictions, predictions, 1).mean()\n",
    "    \n",
    "    return mae - 0.5 * ens_var\n",
    "\n",
    "# Function to calculate transition durations\n",
    "def calculate_transition_durations(y_values, upper_bound, lower_bound):\n",
    "\n",
    "    times_between_transitions = []\n",
    "    transition_start = None\n",
    "    above_upper = False\n",
    "    below_lower = False\n",
    "    for i in range(1, len(y_values)):\n",
    "        if y_values[i] < lower_bound:  \n",
    "            below_lower = True\n",
    "            above_upper = False\n",
    "        elif y_values[i] > upper_bound:  \n",
    "            if below_lower and transition_start is not None:\n",
    "                times_between_transitions.append(i - transition_start)\n",
    "                transition_start = None  \n",
    "            above_upper = True\n",
    "            below_lower = False\n",
    "\n",
    "        if below_lower and transition_start is None:\n",
    "            transition_start = i\n",
    "    return times_between_transitions\n",
    "\n",
    "def KL_coefficient(real_data, pred_mean, delta, cycle, KL_by_dim_cycle):\n",
    "\n",
    "    # Calculating KL divergence\n",
    "    KL_real_data = real_data[:30000, 1, 63]\n",
    "    KL_predictions = pred_mean[:30000]\n",
    "    KL_row = []\n",
    "    \n",
    "    actual_hist, bin_edges = np.histogram(KL_real_data, bins=50, density=True)\n",
    "    pred_hist, _ = np.histogram(pred_mean, bins=bin_edges, density=True)\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    actual_hist += epsilon\n",
    "    pred_hist += epsilon\n",
    "\n",
    "    # Calculate KL divergence between the two histograms\n",
    "    KL = np.sum(actual_hist * np.log(actual_hist / pred_hist))\n",
    "\n",
    "    norm_KL = normalize_transition_time(KL, 1, 0)\n",
    "    print(f\"Normalized KL divergence for delta {delta}, cycle {cycle}: {norm_KL:.6f}\")\n",
    "    KL_by_dim_cycle[delta][cycle].append(norm_KL)\n",
    "\n",
    "    return KL_real_data, KL_predictions, norm_KL\n",
    "\n",
    "def Slope_fit(pred_durations, slope_real, epoch):\n",
    "    # === PREDICTIONS CCDF AND FIT ===\n",
    "    if len(pred_durations) > 0 and len(np.unique(pred_durations)) > 1:\n",
    "        pred_data_sorted = np.sort(pred_durations)\n",
    "        ccdf_pred = 1 - np.arange(1, len(pred_data_sorted) + 1) / len(pred_data_sorted)\n",
    "\n",
    "        valid_indices_pred = ccdf_pred > 0\n",
    "        x_fit_pred = pred_data_sorted[valid_indices_pred]\n",
    "        y_fit_pred = np.log(ccdf_pred[valid_indices_pred])\n",
    "\n",
    "        slope_pred, intercept_pred, *_ = linregress(x_fit_pred, y_fit_pred)\n",
    "        slope_diff_normalized = normalize_transition_time(slope_pred, 0.005, slope_real)\n",
    "        return slope_diff_normalized\n",
    "\n",
    "    else:\n",
    "        print(\"No transitions detected in predictions for CCDF slope evaluation.\")\n",
    "\n",
    "def Mean_and_std_of_predictions(pred_durations, real_durations, delta, cycle, transitions_by_dim_cycle, transitions_normalized_by_dim_cycle, transitions_normalized_std_by_dim_cycle):\n",
    "    transition_mean = np.mean(pred_durations)\n",
    "    transition_std = np.std(pred_durations)\n",
    "\n",
    "    transition_mean_diff = abs(transition_mean - np.mean(real_durations))\n",
    "    transition_std_diff = abs(transition_std - np.std(real_durations))\n",
    "\n",
    "    transition_mean_diff_normalized = normalize_transition_time(transition_mean_diff, 1000, np.mean(real_durations))\n",
    "    transition_std_diff_normalized = normalize_transition_time(transition_std_diff, 1000, np.std(real_durations))\n",
    "\n",
    "    if transition_std_diff_normalized == 0:\n",
    "        transition_std_diff_normalized = 1\n",
    "\n",
    "    transitions_by_dim_cycle[delta][cycle].append(transition_mean_diff)\n",
    "    transitions_normalized_by_dim_cycle[delta][cycle].append(transition_mean_diff_normalized)\n",
    "    transitions_normalized_std_by_dim_cycle[delta][cycle].append(transition_std_diff_normalized)\n",
    "\n",
    "    return transition_mean_diff_normalized, transition_std_diff_normalized\n",
    "\n",
    "# KL Annealing\n",
    "def frange_cycle_linear(start, stop, n_epoch, n_cycle=4, ratio=0.5):\n",
    "    L = np.ones(n_epoch)\n",
    "    period = n_epoch/n_cycle\n",
    "    step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "    for c in range(n_cycle):\n",
    "\n",
    "        v , i = start , 0\n",
    "        while v <=stop and (int(i+c*period) < n_epoch):\n",
    "            L[int(i+c*period)] = v\n",
    "            v += step\n",
    "            i += 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def model_restore(model_path, model):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "def inference(model, psi, mean_psi, std_psi, time_step, variable_range, variable, num_variables, latent_dim):\n",
    "    start, end = variable_range[variable][0], variable_range[variable][1]+1\n",
    "    initial_cond = torch.reshape(torch.tensor(psi[0,start:end]), [1, num_variables])\n",
    "    z = torch.zeros([1,latent_dim])\n",
    "    pred = np.zeros ([time_step, 75])\n",
    "\n",
    "    for k in tqdm(range (0, time_step), desc=\"Inference\"):\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "\n",
    "                if (k ==0):\n",
    "\n",
    "                    z = torch.randn_like(z).float().cuda(non_blocking=True)\n",
    "                    initial_cond = initial_cond.float().cuda(non_blocking=True)\n",
    "\n",
    "                    y = (model.decode(z,initial_cond)).detach().cpu().numpy()\n",
    "                    pred[k,:] = y\n",
    "\n",
    "                    y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                    initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "\n",
    "                else:\n",
    "                    z = torch.randn_like(z).float().cuda(non_blocking=True)\n",
    "                    initial_cond = torch.reshape(torch.tensor(pred[k-1,start:end]),[1,num_variables]).float().cuda(non_blocking=True)\n",
    "\n",
    "                    y = (model.decode(z,initial_cond)).detach().cpu().numpy()\n",
    "                    pred[k,:] = y\n",
    "\n",
    "                    y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                    initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def euclidean_distance_for_predictions(metrics):\n",
    "    sum = 0\n",
    "    for metric in metrics:\n",
    "        sum += metric ** 2\n",
    "    distance = np.sqrt(sum)\n",
    "    return distance\n",
    "\n",
    "def save_best_cycle_epoch(models, delta, cycle, epoch, \n",
    "                          exp_fit_normalized_by_dim_cycle, \n",
    "                          KL_by_dim_cycle, duration_diff_by_dim_cycle, \n",
    "                          best_models_saved, best_models):\n",
    "    \n",
    "    best_model = None\n",
    "    best_index = -1\n",
    "    best_distance = float('inf')\n",
    "\n",
    "    for i in range(len(models)):  # models contains each epoch's model in the current cycle\n",
    "\n",
    "        metrics = [exp_fit_normalized_by_dim_cycle[delta][cycle][i],\n",
    "                   KL_by_dim_cycle[delta][cycle][i], \n",
    "                   duration_diff_by_dim_cycle[delta][cycle][i]]\n",
    "        \n",
    "        distance = euclidean_distance_for_predictions(metrics)\n",
    "        if distance < best_distance:\n",
    "            best_distance = distance\n",
    "            shutil.copyfile(models[i], f\"{folder}/best_model_combined_distance_at_cycle_{cycle}_and_checkpoint_{epoch}.pth\")\n",
    "            print(f\"New best model saved with distance {distance:.4f} at epoch {i+1}\")\n",
    "            best_index = i\n",
    "            best_model = models[i]\n",
    "\n",
    "    if best_index != -1:\n",
    "        best_models_saved.append(best_model)\n",
    "        best_models.append((cycle, best_index))\n",
    "\n",
    "def save_best_epoch(best_models, best_models_saved, exp_fit_normalized_by_dim_cycle,\n",
    "                    KL_by_dim_cycle, duration_diff_by_dim_cycle, delta, master_folder):\n",
    "    \n",
    "    print(\"Selecting the best model based on combined distance...\")\n",
    "\n",
    "    # Ensure best_models is not empty\n",
    "    if not best_models:\n",
    "        print(\"No best models found.\")\n",
    "        return\n",
    "    \n",
    "    # After all cycles - final best model selection\n",
    "    best_model = None\n",
    "    where_model = (-1, -1)\n",
    "    best_model_distance = float('inf')\n",
    "\n",
    "    print(f\"Number of best models saved: {len(best_models)}\")\n",
    "    for idx, (cycle_num, epoch_idx) in enumerate(best_models):\n",
    "\n",
    "        metrics = [exp_fit_normalized_by_dim_cycle[delta][cycle_num][epoch_idx], \n",
    "                   KL_by_dim_cycle[delta][cycle_num][epoch_idx], \n",
    "                   duration_diff_by_dim_cycle[delta][cycle_num][epoch_idx]]\n",
    "        \n",
    "        distance = euclidean_distance_for_predictions(metrics)        \n",
    "        print(f\"Distance for model from cycle {cycle_num+1}, epoch {epoch_idx+1}: {distance:.4f}\")\n",
    "        print(f\"Current best distance: {best_model_distance:.4f}\")\n",
    "        \n",
    "        if distance < best_model_distance:\n",
    "            best_model_distance = distance\n",
    "            best_model = best_models_saved[idx]\n",
    "            where_model = (cycle_num, epoch_idx)\n",
    "\n",
    "    # Save the best model  \n",
    "    i,n = where_model\n",
    "    cycle = i\n",
    "    epoch = n\n",
    "\n",
    "    if cycle == -1:\n",
    "        print(\"No best model found.\")\n",
    "    else:\n",
    "        shutil.copyfile(best_model, f\"{master_folder}/best_model_combined_distance_with_cycle_{cycle+1}_and_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Best model saved with cycle {cycle+1} and epoch {epoch+1}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Timeseries_plot(actual, pred, epoch, ax):\n",
    "\n",
    "    ax.plot(actual, 'b', label='Actual')\n",
    "    ax.plot(pred, 'r', label='Predictions')\n",
    "\n",
    "    ax.set_title(f\"Timeseries | Epoch {epoch}\", fontsize=16)\n",
    "    ax.set_xlabel('Time Step', fontsize=14)\n",
    "    ax.set_ylabel('Zonal Wind Value', fontsize=14)\n",
    "\n",
    "    ax.legend(['Predictions', 'Actual'])\n",
    "    ax.grid(True)\n",
    "\n",
    "    # save_path = os.path.join(folder, \"timeseries\")\n",
    "    # save_path = os.path.join(save_path, f\"timeseries_plot_{epoch+1}.png\")\n",
    "\n",
    "    # plt.savefig(save_path)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def PDF_plot(actual, pred, epoch, pdf_distance, ax):\n",
    "\n",
    "    sns.histplot(actual, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data', ax=ax)\n",
    "    sns.histplot(pred, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions', ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Probability Distribution Functions (PDFs) | Epoch {epoch} | KL Error: {pdf_distance:.4f}\", fontsize=16)\n",
    "    ax.set_xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    \n",
    "    ax.axvline(np.mean(actual), color='black', linestyle='--', label=f'Real Mean: {np.mean(actual):.2f}')\n",
    "    ax.axvline(np.mean(pred), color='red', linestyle='--', label=f'Pred Mean: {np.mean(pred):.2f}')\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # save_path = os.path.join(folder, \"bi_modal_distri\")\n",
    "    # save_path = os.path.join(save_path, f\"bi_modal_distribution_plot_{epoch+1}.png\")\n",
    "    # plt.savefig(save_path)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def Exp_fit_plot(x_line_pred, y_values_pred, x_line_real, y_values_real, exponential_fit_pred, exponential_fit_real, epoch, exp_distance, range_distance, ax):\n",
    "\n",
    "    ax.plot(x_line_pred, y_values_pred, 'r-', label=f'Pred Exp Fit (slope={exponential_fit_pred:.4f})', linewidth=2)\n",
    "    ax.plot(x_line_real, y_values_real, 'b-', label=f'Real Exp Fit (slope={exponential_fit_real:.4f})', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Time Duration (Steps)')\n",
    "    ax.set_ylabel('Exponential Fit')\n",
    "    ax.set_title(f\"Exponential Fits of Transition Return Periods | Epoch {epoch} | Exp Error: {exp_distance:.4f} | Range Error: {range_distance:.4f}\", fontsize=16)\n",
    "   \n",
    "    ax.set_yscale(\"linear\")  # y-axis log scale\n",
    "    ax.set_xscale(\"linear\")  # x-axis linear scale\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    \n",
    "    # save_path = os.path.join(folder, \"expo_fit\")\n",
    "    # save_path = os.path.join(save_path, f\"expo_fit_plot_{epoch}.png\")\n",
    "    # plt.savefig(save_path)\n",
    "    # plt.show()\n",
    "\n",
    "def Final_avg_transition_plot(transitions_by_dim_cycle, transition_real, delta, num_cycles, folder):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for c in range(num_cycles):\n",
    "        plt.plot(transitions_by_dim_cycle[delta][c], 'o-', label=f'Cycle {c}')\n",
    "    \n",
    "    plt.axhline(y=transition_real, color='r', linestyle='--', label='Real Data')\n",
    "\n",
    "    plt.xlabel('Epoch within Cycle')\n",
    "    plt.ylabel('Average Transition Value')\n",
    "    plt.ylim(0.1,2000)\n",
    "    plt.title(f'Average Transition Progress (Delta Coefficient={delta})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    save_path = os.path.join(folder, \"summary\")\n",
    "    save_path = os.path.join(save_path, f\"transition_plot_all_cycles.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def Final_exp_fit_plot(exp_fit_by_dim_cycle, exponential_fit_real, delta, num_cycles, folder):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for c in range(num_cycles):\n",
    "        plt.plot(exp_fit_by_dim_cycle[delta][c], 'o-', label=f'Cycle {c}')\n",
    "    \n",
    "    plt.axhline(y=exponential_fit_real, color='r', linestyle='--', label='Real Data')\n",
    "\n",
    "    plt.xlabel('Epoch within Cycle')\n",
    "    plt.ylabel('Exponential Fit Value')\n",
    "    plt.title(f'Exponential Fit Progress (Delta Coefficient={delta})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    save_path = os.path.join(folder, \"summary\")\n",
    "    save_path = os.path.join(save_path, f\"exponential_fit_plot_all_cycles.png\")\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def all_plot(actual, pred, x_line_pred, y_values_pred, x_line_real, y_values_real, \n",
    "             exponential_fit_pred, exponential_fit_real, pdf_distance, exp_distance, range_distance, epoch, folder):\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig, width_ratios=[1,1], height_ratios=[1,1])\n",
    "\n",
    "    ax_timmeseries = fig.add_subplot(gs[:, 0])\n",
    "    ax_pdf = fig.add_subplot(gs[0, 1])\n",
    "    ax_exp_fit = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    Timeseries_plot(actual, pred, epoch, ax_timmeseries)\n",
    "    PDF_plot(actual, pred, epoch, pdf_distance, ax_pdf)\n",
    "    Exp_fit_plot(x_line_pred, y_values_pred, x_line_real, y_values_real, \n",
    "                 exponential_fit_pred, exponential_fit_real, epoch, exp_distance, range_distance, ax_exp_fit)\n",
    "    \n",
    "    distance = np.sqrt(pdf_distance**2 + exp_distance**2 + range_distance**2)\n",
    "    fig.suptitle(f\"Predictions vs Actual | Epoch {epoch} | Euclidean Metric Error: {distance}\", fontsize=20)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.35)  # Adjust these values as desired\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    plt.savefig(os.path.join(folder, f\"plots/all_plots_epoch_{epoch+1}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def Loss_plot(losses_training, losses_validation, cycle, delta, folder):\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    plt.plot(losses_training, label='Training Loss')\n",
    "    plt.plot(losses_validation, label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Losses for Cycle {cycle+1} with Delta {delta}')\n",
    "    plt.legend()\n",
    "\n",
    "    # save_path = os.path.join(folder, \"summary\")\n",
    "    # save_path = os.path.join(save_path, f\"loss_plot_cycle_{cycle+1}_delta_{delta}.png\")\n",
    "    # plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_kl_coef = frange_cycle_linear(0.01, 0.3, 500, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d521812",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Initialization\n",
    "scaler          = GradScaler()\n",
    "\n",
    "latent_dim      = 1024\n",
    "kl_coefficients = [0.1]\n",
    "kl_coef         = 2\n",
    "delta_coefs     = [1]\n",
    "time_step       = 30000\n",
    "num_cycles      = 1\n",
    "ens_size        = 1\n",
    "level           = 63\n",
    "upper_bound     = 53.8 / 2.8935\n",
    "lower_bound     = 7.41\n",
    "learning_rate   = .00005\n",
    "TRAIN_N         = 250000\n",
    "VAL_N           = 50000\n",
    "num_epochs      = 200\n",
    "batch_size      = 1024\n",
    "\n",
    "run = wandb.init(entity=\"fabio2-the-university-of-chicago\", project=\"ssw_research\",\n",
    "        config={\n",
    "        \"architecture\": \"VAE\",\n",
    "        \"dataset\": \"Holton-Mass\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"latent_dim size\" : latent_dim,\n",
    "        \"KL list\" : True,\n",
    "        \"kl_coef\" : kl_coef,\n",
    "        \"ensemble size\" : ens_size,\n",
    "        \"Neurons @ Deep layer\" : num_neurons\n",
    "        },\n",
    ")\n",
    "\n",
    "restore         = True\n",
    "\n",
    "best_distance   = float('inf')\n",
    "\n",
    "real_data = np.load(r'/home/fabio/work/HM_and_AI_models/VAE_Model/data/long_run_310k.npy')\n",
    "\n",
    "real_data_1d    = real_data[:, 1, level]\n",
    "\n",
    "# Function to calculate transition durations\n",
    "real_durations      = calculate_transition_durations(real_data_1d, upper_bound, lower_bound)\n",
    "real_data_sorted    = np.sort(real_durations)\n",
    "transition_real     = np.mean(real_data_sorted)\n",
    "\n",
    "actual_hist, bin_edges = np.histogram(real_data[:, 1, level], bins=50, density=True)\n",
    "print(f\"Reference Real Data average_transition_time: {transition_real}\")\n",
    "\n",
    "# Compute CCDF slope for real data\n",
    "ccdf_real           = 1 - np.arange(1, len(real_data_sorted) + 1) / len(real_data_sorted)\n",
    "valid_indices_real  = ccdf_real > 0\n",
    "x_fit_real          = real_data_sorted[valid_indices_real]\n",
    "y_fit_real          = np.log(ccdf_real[valid_indices_real])\n",
    "slope_real, intercept_real, *_ = linregress(x_fit_real, y_fit_real)\n",
    "print(f\"Reference Real Data CCDF Slope: {slope_real}\")\n",
    "\n",
    "# Compute exponential fit for real data\n",
    "x_line_real = np.linspace(min(real_data_sorted), max(real_data_sorted), 100)\n",
    "exponential_fit_real = 1/np.mean(real_data_sorted)\n",
    "y_values_real = exponential_fit_real*x_line_real\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "transitions_by_dim_cycle                = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "transitions_normalized_by_dim_cycle     = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "transitions_normalized_std_by_dim_cycle = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "duration_diff_by_dim_cycle              = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "slope_diff_by_dim_cycle                 = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "exp_fit_by_dim_cycle                    = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "exp_fit_normalized_by_dim_cycle         = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "KL_by_dim_cycle                         = {dl: {cycle: [] for cycle in range(num_cycles)} for dl in delta_coefs}\n",
    "\n",
    "models_by_dim_cycle = []\n",
    "\n",
    "master_folder = f'/home/fabio/work/HM_and_AI_models/VAE_Model/data/save_folder/{datetime.datetime.now()}'\n",
    "os.makedirs(master_folder)\n",
    "\n",
    "for delta in delta_coefs:\n",
    "    print(f\"USING DELTA COEF OF {delta}\")\n",
    "\n",
    "    best_models         = []\n",
    "    best_models_saved   = []\n",
    "\n",
    "    losses_training     = []\n",
    "    losses_validation   = []\n",
    "\n",
    "    for cycle in range(0,num_cycles):\n",
    "\n",
    "        models = []\n",
    "\n",
    "        # Initialize the model, optimizer, and loss function\n",
    "        model       = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "        model       = model.cuda()\n",
    "        optimizer   = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create the folder structure for saving results\n",
    "        subfolders  = ['plots','checkpoints']\n",
    "        folder      = f\"{master_folder}/model_at_{cycle}_with_delta_{delta}\"\n",
    "        os.makedirs(folder)\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(folder, subfolder)\n",
    "            os.mkdir(path)\n",
    "\n",
    "        # Restore the model if specified\n",
    "        # if restore:\n",
    "        #     model_path = \"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/Resnet_VAE_model_DELTA_TEST_base_1000_epoch/model_at_0_with_delta_1/checkpoint_1000\"\n",
    "        #     model_restore(model_path, model)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(range (0, TRAIN_N, batch_size), desc=f\"Training loop @ {epoch + 1}\"):\n",
    "                input_batch = psi_train_input[batch:batch + batch_size,:]\n",
    "                label_batch = psi_train_label[batch:batch + batch_size,:]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs, mus, logvars = zip(*[model(label_batch.float().cuda(non_blocking=True), input_batch.float().cuda(non_blocking=True)) for _ in range(ens_size)])\n",
    "                    output = torch.stack(outputs)\n",
    "                    mu = torch.stack(mus)\n",
    "                    logvar = torch.stack(logvars)\n",
    "\n",
    "                    reconstruction_loss = F.smooth_l1_loss(output, label_batch.float().cuda(non_blocking=True), reduction=\"mean\")\n",
    "                    crps_loss = crps_score(output, label_batch.float().cuda(non_blocking=True)) * 0.0001\n",
    "                    kl_loss = 0.5 * (mu ** 2 + torch.exp(logvar) - 1 - logvar).sum() * beta_kl_coef[epoch]\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = reconstruction_loss + kl_loss + 0*crps_loss\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                \n",
    "            losses_training.append(loss.item())\n",
    "            print(f'''Epoch {epoch+1}, \n",
    "                Reconstrunction Loss: {reconstruction_loss.item()}\n",
    "                KL Loss: {kl_loss.item()}\n",
    "                CRPS Loss: {crps_loss.item()}\n",
    "                Total Loss: {loss.item()}\n",
    "                ''')\n",
    "\n",
    "            #Training Loss \n",
    "            run.log({\"Loss\": reconstruction_loss, \"KL-Loss\": kl_loss, \n",
    "                     \"CRPS Loss\": crps_loss})\n",
    "\n",
    "            # Validation Loss\n",
    "            for batch in tqdm(range (0, VAL_N, batch_size), desc=f\"Validation loop @ {epoch + 1}\"):\n",
    "\n",
    "                model.eval()\n",
    "                input_batch = psi_val_input[batch:batch + batch_size,:]\n",
    "                label_batch = psi_val_label[batch:batch + batch_size,:]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        outputs, mus, logvars = zip(*[model(label_batch.float().cuda(non_blocking=True), input_batch.float().cuda(non_blocking=True)) for _ in range(ens_size)])\n",
    "                        output = torch.stack(outputs)\n",
    "                        mu = torch.stack(mus)\n",
    "                        logvar = torch.stack(logvars)\n",
    "\n",
    "                        val_reconstruction_loss = F.smooth_l1_loss(output, label_batch.float().cuda(non_blocking=True), reduction=\"mean\")\n",
    "                        crps_loss = crps_score(output, label_batch.float().cuda(non_blocking=True)) * 0.0001\n",
    "                        kl_loss = 0.5 * (mu ** 2 + torch.exp(logvar) - 1 - logvar).sum() * beta_kl_coef[epoch]  \n",
    "                \n",
    "                # Total loss\n",
    "                val_loss = val_reconstruction_loss + kl_loss + 0*crps_loss\n",
    "\n",
    "            losses_validation.append(val_loss.item())\n",
    "            print(f'''\n",
    "                Validation Reconstrunction Loss: {val_reconstruction_loss.item()}\n",
    "                Validation KL Loss: {kl_loss.item()}\n",
    "                Validation CRPS Loss: {crps_loss.item()}\n",
    "                Validation Total Loss: {val_loss.item()}''')\n",
    "\n",
    "            # WandB Log \n",
    "            run.log({\"Val Loss\": val_reconstruction_loss, \n",
    "                     \"Val KL-Loss\" : kl_loss, \"Val CRPS-Loss\": crps_loss})\n",
    "\n",
    "\n",
    "            # Inference\n",
    "            pred = inference(model, psi, mean_psi, std_psi, \n",
    "                             time_step, variable_range, variable, \n",
    "                             num_variables, latent_dim)\n",
    "                \n",
    "            # Denormalize final preds\n",
    "            pred_mean = pred[:time_step, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "            actual_values = psi[:time_step, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "            predictions_1d = pred_mean[:, 63]  # shape (300000,)\n",
    "           \n",
    "            # Calculate transition durations for predictions\n",
    "            pred_durations = calculate_transition_durations(predictions_1d, \n",
    "                                                            upper_bound, \n",
    "                                                            lower_bound)\n",
    "\n",
    "            # Calculate KL coefficient\n",
    "            KL_real_data, KL_predictions, avg_norm_KL = KL_coefficient(real_data, \n",
    "                                                                           predictions_1d, \n",
    "                                                                           delta, \n",
    "                                                                           cycle, \n",
    "                                                                           KL_by_dim_cycle)\n",
    "            \n",
    "            # Calculate mean and standard deviation of predictions\n",
    "            transition_diff_normalized, transition_std_diff_normalized = Mean_and_std_of_predictions(pred_durations, \n",
    "                                                                                                     real_durations, \n",
    "                                                                                                     delta, cycle,\n",
    "                                                                                                     transitions_by_dim_cycle,\n",
    "                                                                                                     transitions_normalized_by_dim_cycle,\n",
    "                                                                                                     transitions_normalized_std_by_dim_cycle)\n",
    "\n",
    "            # Calculate the difference between exponential fits\n",
    "            slope_diff_normalized = 1\n",
    "            slope_diff_normalized = Slope_fit(pred_durations, slope_real, epoch)\n",
    "            slope_diff_by_dim_cycle[delta][cycle].append(slope_diff_normalized)\n",
    "\n",
    "            # Initialize variables for exponential fit and range of transitions\n",
    "            exponential_fit_pred = 0\n",
    "            duration_diff_normalized = 1\n",
    "\n",
    "            if len(pred_durations) > 0 and  len(np.unique(pred_durations)) > 1:\n",
    "\n",
    "                # Calculate the exponential fit for predictions\n",
    "                x_line_pred = np.linspace(min(pred_durations), max(pred_durations), 100)\n",
    "                exponential_fit_pred = 1/np.mean(pred_durations)\n",
    "                exp_fit_by_dim_cycle[delta][cycle].append(exponential_fit_pred)\n",
    "                exp_fit_normalized = normalize_transition_time(exponential_fit_pred, 0.005, exponential_fit_real)\n",
    "                print(f\"Exponential Fit Normalized: {exp_fit_normalized:.6f}\")\n",
    "\n",
    "                # Calculate the y-values for the exponential fit\n",
    "                y_values_pred = exponential_fit_pred*x_line_pred\n",
    "\n",
    "                # Calculate the range of transitions\n",
    "                max_pred = np.max(pred_durations)\n",
    "                min_pred = np.min(pred_durations)\n",
    "                \n",
    "                if max_pred > 0:\n",
    "\n",
    "                    # Calculate the predicted range and normalize it\n",
    "                    difference = abs(max_pred - min_pred)\n",
    "                    duration_diff_normalized = normalize_transition_time(difference, 10000, abs(np.max(real_durations)-np.min(real_durations)))\n",
    "                    duration_diff_by_dim_cycle[delta][cycle].append(duration_diff_normalized)\n",
    "                    print(f\"Duration Difference Normalized: {duration_diff_normalized:.6f}\")\n",
    "                    all_plot(actual_values[:30000, 63],\n",
    "                                predictions_1d[:30000], \n",
    "                                x_line_pred, y_values_pred, \n",
    "                                x_line_real, y_values_real, \n",
    "                                exponential_fit_pred, exponential_fit_real, avg_norm_KL,\n",
    "                                exp_fit_normalized, duration_diff_normalized,\n",
    "                                epoch+1, folder)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"No distribution of transitions detected in predictions.\")\n",
    "            else:\n",
    "                exp_fit_by_dim_cycle[delta][cycle].append(exponential_fit_pred)\n",
    "                duration_diff_by_dim_cycle[delta][cycle].append(duration_diff_normalized)\n",
    "                exp_fit_normalized = normalize_transition_time(exponential_fit_pred, 0.005, exponential_fit_real)\n",
    "                exp_fit_normalized_by_dim_cycle[delta][cycle].append(exp_fit_normalized)\n",
    "                print(\"No transitions detected in predictions for exponential fit evaluation.\")\n",
    "\n",
    "            # Calculate accuracy by euclidean distance with specified metrics\n",
    "            metrics = [exp_fit_normalized, avg_norm_KL, duration_diff_normalized]\n",
    "            distance = euclidean_distance_for_predictions(metrics)\n",
    "            print(f\"Epoch {epoch+1}: Exponential Transition Fit Predictions Normalized: {exp_fit_normalized}, KL Normalized = {avg_norm_KL}, Duration Difference Normalized = {duration_diff_normalized}, Combined Distance = {distance:.6f}\")\n",
    "            \n",
    "            # Save the model weights at each epoch\n",
    "            path = f\"{folder}/checkpoints/checkpoint_{epoch+1}\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f\"Model weights saved to {folder} with point {epoch+1}.\")\n",
    "            models.append(path)\n",
    "\n",
    "            #Validation Loss\n",
    " \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea14b56",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713280b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "latent_dim = 1024\n",
    "output_dim = 75\n",
    "condition_dim = num_variables\n",
    "model = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "start, end = variable_range[variable][0], variable_range[variable][1]+1\n",
    "\n",
    "model_weights_path = r'/home/fabio/work/HM_and_AI_models/VAE_Model/data/save_folder/2025-07-02 10:41:38.136471/model_at_0_with_delta_1/checkpoints/checkpoint_7'\n",
    "past_model = True\n",
    "\n",
    "print(\"Before past_model\")\n",
    "if past_model:\n",
    "    if os.path.exists(model_weights_path):\n",
    "        print(\"In past model\")\n",
    "        model.load_state_dict(torch.load(model_weights_path))\n",
    "        print(f\"Model weights loaded from {model_weights_path}.\")\n",
    "\n",
    "for _ in range (0,1):\n",
    "\n",
    "    start, end = variable_range[variable][0], variable_range[variable][1]+1\n",
    "    initial_cond = torch.reshape(torch.tensor(psi[0,start:end]), [1, num_variables])\n",
    "    print(initial_cond.shape)\n",
    "    time_step = 30_000\n",
    "    z = torch.zeros([1,latent_dim])\n",
    "    num_ens = 1\n",
    "    pred = np.zeros ([time_step, 75, num_ens])\n",
    "\n",
    "    for k in tqdm(range (0, time_step), desc= f\"Inference progress\"):\n",
    "\n",
    "        for ens in range (0, num_ens):\n",
    "            if (k ==0):\n",
    "                z = torch.randn_like(z)\n",
    "                print(z.shape, initial_cond.shape)\n",
    "                y = (model.decode(z.float().cuda(non_blocking=True),initial_cond.float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "                pred[k,:,ens] = y\n",
    "                y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "\n",
    "            else:\n",
    "                select_ens = np.random.randint(0,num_ens,1)\n",
    "                z = torch.randn_like(z)\n",
    "                y = (model.decode(z.float().cuda(non_blocking=True),torch.reshape(torch.tensor(pred[k-1,start:end,select_ens]),[1,num_variables]).float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "                pred[k,:, ens] = y\n",
    "                y_denorm_contracted = (y[:, start:end] * std_psi[:, start:end] + mean_psi[:, start:end])\n",
    "                initial_cond = torch.tensor((y_denorm_contracted[:, start:end] - mean_psi[:, start:end]) / std_psi[:, start:end])\n",
    "\n",
    "    # Denormalize final preds\n",
    "    pred = pred.reshape(pred.shape[0], pred.shape[1])\n",
    "    pred_mean = pred[:, :] * std_psi[:, :] + mean_psi[:, :]\n",
    "    \n",
    "    # Denormalize test labels\n",
    "    actual_values = psi[:, :] * std_psi[:, :] + mean_psi[:, :]\n",
    " \n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(pred_mean[0:30000, 63],'r')\n",
    "    plt.plot(actual_values[0:30000, 63],'b')\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Predictions vs Actual\")\n",
    "    plt.show()\n",
    "    np.save(f'/home/fabio/work/HM_and_AI_models/VAE_Model/data/best_pred', pred_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd62a0c",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94752682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAGS to determine testing\n",
    "plot_data = 1\n",
    "#what level do you want to plot\n",
    "level = 63\n",
    "CCDF = 1\n",
    "Bi_modal_distribution = 1\n",
    "single_step_profiles = 1\n",
    "#for the single_step_profiles\n",
    "NUM_SAMPLES = 5\n",
    "#what weights do you want to use?\n",
    "\n",
    "LEVEL = 63\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Load the data; shape = (300000, 2, 75)\n",
    "real_data = actual_values\n",
    "predictions = np.load('/home/fabio/work/HM_and_AI_models/VAE_Model/data/best_pred.npy')\n",
    "\n",
    "#reshape the predictions so that it matches the real_data shape\n",
    "print(predictions.shape)\n",
    "print(real_data.shape)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "folder = f\"testing_at_{timestamp}\"\n",
    "os.mkdir(folder)\n",
    "subfolders = ['timeseries', 'CCDF', 'bi_modal_distribution', 'single_step_profiles']\n",
    "\n",
    "# Create each subdirectory inside the main folder\n",
    "for subfolder in subfolders:\n",
    "    path = os.path.join(folder, subfolder)\n",
    "    os.mkdir(path)\n",
    "    print(f\"Created subfolder: {path}\")\n",
    "SAVE_DIR = os.path.join(folder, \"single_step_profiles\")\n",
    "\n",
    "model = ConditionalVAE(latent_dim, output_dim, condition_dim)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "if os.path.exists(model_weights_path):\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    print(f\"Model weights loaded from {model_weights_path}.\")\n",
    "    \n",
    "if (plot_data):\n",
    "    #note that the value 300000 will have to change depending on the real and predictions data length\n",
    "    u_profile_real = real_data[:30000, level]  # Match time length with predictions\n",
    "    u_profile_pred = predictions[:, level]\n",
    "    time_steps = np.arange(len(u_profile_pred))\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.plot(time_steps, u_profile_real, label='Real Data', alpha=0.7)\n",
    "    plt.plot(time_steps, u_profile_pred, label='Predictions', linestyle='--')\n",
    "\n",
    "\n",
    "    # Labels, legend, and formatting\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('U (m/s)')\n",
    "    plt.title(f'Time Series of U at Vertical Level {level}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"timeseries\")\n",
    "    save_path = os.path.join(save_path, \"real_prediction_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "if (CCDF):\n",
    "    real_data_1d = real_data[:, 63]  # Now shape is (309700,)\n",
    "    predictions_1d = predictions[:, 63]  # shape (300000,)\n",
    "\n",
    "    # Define bounds (assuming they apply to both datasets)\n",
    "    upper_bound = 53.8 / 2.8935\n",
    "    lower_bound = 7.41\n",
    "\n",
    "    # Function to calculate transition durations\n",
    "    def calculate_transition_durations(y_values, upper_bound, lower_bound):\n",
    "        times_between_transitions = []\n",
    "        transition_start = None\n",
    "        above_upper = False\n",
    "        below_lower = False\n",
    "\n",
    "        for i in range(1, len(y_values)):\n",
    "            if y_values[i] < lower_bound:  \n",
    "                below_lower = True\n",
    "                above_upper = False\n",
    "            elif y_values[i] > upper_bound:  \n",
    "                if below_lower and transition_start is not None:\n",
    "                    times_between_transitions.append(i - transition_start)\n",
    "                    transition_start = None  \n",
    "                above_upper = True\n",
    "                below_lower = False\n",
    "\n",
    "            if below_lower and transition_start is None:\n",
    "                transition_start = i\n",
    "\n",
    "        return times_between_transitions\n",
    "\n",
    "    # Compute transition durations for real data\n",
    "    real_durations = calculate_transition_durations(real_data_1d, upper_bound, lower_bound)\n",
    "\n",
    "    # Compute transition durations for predictions data\n",
    "    pred_durations = calculate_transition_durations(predictions_1d, upper_bound, lower_bound)\n",
    "\n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # === REAL DATA CCDF AND FIT ===\n",
    "    if len(real_durations) == 0:\n",
    "        print(\"No transitions detected in real data with current bounds!\")\n",
    "    else:\n",
    "        real_data_sorted = np.sort(real_durations)\n",
    "        x_line_real = np.linspace(min(real_data_sorted), max(real_data_sorted), 100)\n",
    "        exponential_fit_real = 1/np.mean(real_data_sorted)\n",
    "        y_values_real = exponential_fit_real*x_line_real\n",
    "        plt.plot(x_line_real, y_values_real, 'b-', label=f'Real Exp Fit (slope={exponential_fit_real:.4f})', linewidth=2)\n",
    "\n",
    "    # === PREDICTIONS CCDF AND FIT ===\n",
    "    if len(pred_durations) == 0:\n",
    "        print(\"No transitions detected in predictions with current bounds!\")\n",
    "    else:\n",
    "        pred_data_sorted = np.sort(pred_durations)\n",
    "        x_line_pred = np.linspace(min(pred_data_sorted), max(pred_data_sorted), 100)\n",
    "        exponential_fit_pred = 1/np.mean(pred_data_sorted)\n",
    "        y_values_pred = exponential_fit_pred*x_line_pred\n",
    "        plt.plot(x_line_pred, y_values_pred, 'r-', label=f'Pred Exp Fit (slope={exponential_fit_pred:.4f})', linewidth=2)\n",
    "\n",
    "    print(1/np.mean(real_data_sorted))\n",
    "    print(1/np.mean(pred_data_sorted))\n",
    "    # Plot labels and formatting\n",
    "    plt.xlabel('Time Duration (Steps)')\n",
    "    plt.ylabel('CCDF')\n",
    "    plt.title('CCDF of Time Between B->A and A->B Transitions (Exponential Fit)')\n",
    "    plt.yscale(\"linear\")  # y-axis log scale\n",
    "    plt.xscale(\"linear\")  # x-axis linear scale\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"CCDF\")\n",
    "    save_path = os.path.join(save_path, \"CCDF_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "if (Bi_modal_distribution):\n",
    "    zonal_wind_data_real = real_data[:, 63]  # variable index 1 (e.g., zonal wind), level 60\n",
    "    zonal_wind_data_predictions = predictions[:, 63]  # variable index 0 (predictions), level 60\n",
    "\n",
    "    print(f\"Shape of zonal_wind_data_real: {zonal_wind_data_real.shape}\")\n",
    "    print(f\"Shape of zonal_wind_data_predictions: {zonal_wind_data_predictions.shape}\")\n",
    "\n",
    "    # Plot the bimodal histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create histograms (overlaid)\n",
    "    sns.histplot(zonal_wind_data_real, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data')\n",
    "    sns.histplot(zonal_wind_data_predictions, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions')\n",
    "\n",
    "    # Customize plot labels and title\n",
    "    plt.title('Distribution of Zonal Winds For Real Data and Predictions', fontsize=16)\n",
    "    plt.xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "    # Add vertical lines at means\n",
    "    plt.axvline(np.mean(zonal_wind_data_real), color='black', linestyle='--', label=f'Real Mean: {np.mean(zonal_wind_data_real):.2f}')\n",
    "    plt.axvline(np.mean(zonal_wind_data_predictions), color='red', linestyle='--', label=f'Pred Mean: {np.mean(zonal_wind_data_predictions):.2f}')\n",
    "\n",
    "    # Final plot settings\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"bi_modal_distribution\")\n",
    "    save_path = os.path.join(save_path, \"bi_modal_distribution_plot\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "if (single_step_profiles):\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # === Load model weights ===\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    model.eval()\n",
    "\n",
    "    # === Randomly sample time points from real data ===\n",
    "    time_indices = random.sample(range(0, real_data.shape[0] - 2), NUM_SAMPLES)\n",
    "    print(f\"Randomly sampled time steps: {time_indices}\")\n",
    "\n",
    "    # === Time series visualization ===\n",
    "    real_data_timeseries = real_data[:, LEVEL]\n",
    "    time_steps_all = np.arange(len(real_data_timeseries))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_steps_all, real_data_timeseries, label=\"Real Data at Level 61\", color='blue')\n",
    "\n",
    "    # Mark sample points\n",
    "    for idx_num, idx in enumerate(time_indices):\n",
    "        plt.axvline(x=idx, color='green', linestyle='--', linewidth=2)\n",
    "    if len(time_indices) > 0:\n",
    "        plt.axvline(x=time_indices[0], color='green', linestyle='--', linewidth=2, label='Sampled Points')\n",
    "\n",
    "    plt.title(\"Real Data Time Series with Sampled Points Highlighted\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"U (m/s) at Level 61\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(SAVE_DIR, \"real_data_timeseries_with_samples.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "    # === Iterate over each sampled time point ===\n",
    "    for i, time_step in enumerate(time_indices):\n",
    "        next_time_step = time_step + 1\n",
    "\n",
    "        # === Real data: current and next ===\n",
    "        real_current = real_data[time_step, :]       \n",
    "        real_next = real_data[next_time_step, :]      \n",
    "\n",
    "        # === Normalize real_current and make prediction for next step ===\n",
    "        initial_cond = torch.reshape(torch.tensor(psi[time_step,start:end]), [1, num_variables])\n",
    "        z = torch.zeros([1,latent_dim])\n",
    "        num_ens = 1\n",
    "        pred = np.zeros ([time_step, 75, num_ens])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn_like(z)\n",
    "            print(z.shape, initial_cond.shape)\n",
    "            y = (model.decode(z.float().cuda(non_blocking=True),initial_cond.float().cuda(non_blocking=True))).detach().cpu().numpy()\n",
    "\n",
    "        # === Denormalize predicted next ===\n",
    "        pred_next_denorm = y.squeeze() * std_psi.squeeze() + mean_psi.squeeze()\n",
    "\n",
    "        # === Extract U, Re(Psi), Im(Psi) components ===\n",
    "        # U profiles\n",
    "        U_current_real = real_current[51:74]\n",
    "        U_next_real = real_next[51:74]\n",
    "        U_next_pred = pred_next_denorm[51:74]\n",
    "\n",
    "        # Re(Psi) profiles\n",
    "        RePsi_current_real = real_current[0:24]\n",
    "        RePsi_next_real = real_next[0:24]\n",
    "        RePsi_next_pred = pred_next_denorm[0:24]\n",
    "\n",
    "        # Im(Psi) profiles\n",
    "        ImPsi_current_real = real_current[25:50]\n",
    "        ImPsi_next_real = real_next[25:50]\n",
    "        ImPsi_next_pred = pred_next_denorm[25:50]\n",
    "\n",
    "        # === Differences ===\n",
    "        U_diff_real = U_next_real - U_current_real\n",
    "        U_diff_pred = U_next_pred - U_current_real\n",
    "\n",
    "        RePsi_diff_real = RePsi_next_real - RePsi_current_real\n",
    "        RePsi_diff_pred = RePsi_next_pred - RePsi_current_real\n",
    "\n",
    "        ImPsi_diff_real = ImPsi_next_real - ImPsi_current_real\n",
    "        ImPsi_diff_pred = ImPsi_next_pred - ImPsi_current_real\n",
    "\n",
    "        # === Create a single figure with 3 rows (U, Re(Psi), Im(Psi)) ===\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # 3 rows, 2 columns (Profile and Difference)\n",
    "\n",
    "        z_levels_U = np.linspace(0, 70, 23)\n",
    "        z_levels_RePsi = np.linspace(0, 70, 24)\n",
    "        z_levels_ImPsi = np.linspace(0, 70, 25)\n",
    "\n",
    "        # --- U ---\n",
    "        axes[0, 0].plot(U_current_real, z_levels_U, 'x-', label=\"Real Current\")\n",
    "        axes[0, 0].plot(U_next_real, z_levels_U, 'd-', label=\"Real Next\")\n",
    "        axes[0, 0].plot(U_next_pred, z_levels_U, 's--', label=\"Predicted Next\")\n",
    "        axes[0, 0].set_title(f\"U Profiles @ Step {time_step}\")\n",
    "        axes[0, 0].set_xlabel(\"U (m/s)\")\n",
    "        axes[0, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        axes[0, 1].plot(U_diff_real, z_levels_U, 'xb', label=\"Real Δ (Next - Current)\")\n",
    "        axes[0, 1].plot(U_diff_pred, z_levels_U, 'o--r', label=\"Pred Δ (Next - Current)\")\n",
    "        axes[0, 1].set_title(\"U Difference (Next - Current)\")\n",
    "        axes[0, 1].set_xlabel(\"ΔU (m/s)\")\n",
    "        axes[0, 1].legend()\n",
    "\n",
    "        # --- Re(Psi) ---\n",
    "        axes[1, 0].plot(RePsi_current_real, z_levels_RePsi, 'x-', label=\"Real Current\")\n",
    "        axes[1, 0].plot(RePsi_next_real, z_levels_RePsi, 'd-', label=\"Real Next\")\n",
    "        axes[1, 0].plot(RePsi_next_pred, z_levels_RePsi, 's--', label=\"Predicted Next\")\n",
    "        axes[1, 0].set_title(f\"Re(Psi) Profiles @ Step {time_step}\")\n",
    "        axes[1, 0].set_xlabel(\"Re(Psi)\")\n",
    "        axes[1, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[1, 0].legend()\n",
    "\n",
    "        axes[1, 1].plot(RePsi_diff_real, z_levels_RePsi, 'xb', label=\"Real Δ (Next - Current)\")\n",
    "        axes[1, 1].plot(RePsi_diff_pred, z_levels_RePsi, 'o--r', label=\"Pred Δ (Next - Current)\")\n",
    "        axes[1, 1].set_title(\"Re(Psi) Difference (Next - Current)\")\n",
    "        axes[1, 1].set_xlabel(\"ΔRe(Psi)\")\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        # --- Im(Psi) ---\n",
    "        axes[2, 0].plot(ImPsi_current_real, z_levels_ImPsi, 'x-', label=\"Real Current\")\n",
    "        axes[2, 0].plot(ImPsi_next_real, z_levels_ImPsi, 'd-', label=\"Real Next\")\n",
    "        axes[2, 0].plot(ImPsi_next_pred, z_levels_ImPsi, 's--', label=\"Predicted Next\")\n",
    "        axes[2, 0].set_title(f\"Im(Psi) Profiles @ Step {time_step}\")\n",
    "        axes[2, 0].set_xlabel(\"Im(Psi)\")\n",
    "        axes[2, 0].set_ylabel(\"Vertical Levels (km)\")\n",
    "        axes[2, 0].legend()\n",
    "\n",
    "        axes[2, 1].plot(ImPsi_diff_real, z_levels_ImPsi, 'xb', label=\"Real Δ (Next - Current)\")\n",
    "        axes[2, 1].plot(ImPsi_diff_pred, z_levels_ImPsi, 'o--r', label=\"Pred Δ (Next - Current)\")\n",
    "        axes[2, 1].set_title(\"Im(Psi) Difference (Next - Current)\")\n",
    "        axes[2, 1].set_xlabel(\"ΔIm(Psi)\")\n",
    "        axes[2, 1].legend()\n",
    "\n",
    "        # === Finalize and Save ===\n",
    "        plt.suptitle(f\"Single Step Profile Comparisons at Time Step {time_step}\", fontsize=18)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, f\"Profile_Summary_point_{time_step}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Combined plot saved for sampled point {time_step}\")\n",
    "\n",
    "    # Final debug\n",
    "    print(\"Finished processing all sampled points.\")\n",
    "        # Debugging prints\n",
    "    print(predictions.shape) \n",
    "    print(real_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45e5f2",
   "metadata": {},
   "source": [
    "# WHERE ARE WE DOING THE WORST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def total_variation_distance(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "predictions_1d = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/training_cycles/resnet/predictions_best_checkpoint_and_cycle_Resnet_VAE_1.npy\")\n",
    "real_data_1d = np.load(r\"/home/constantino-daniel-boscu/Documents/research/AI-RES/modified-code-main3/data/actual/long_run_310k.npy\")\n",
    "print(predictions_1d.shape, real_data_1d.shape)\n",
    "\n",
    "predictions_1d = predictions_1d.reshape(predictions_1d.shape[0], 1, predictions_1d.shape[1])  # Reshape to (300000, 1, 75)\n",
    "\n",
    "# Flatten the data to 1D arrays\n",
    "predictions_1d = predictions_1d[:, 0, :]  # Extracting the 63rd variable (e.g., zonal wind)\n",
    "real_data_1d = real_data_1d[:, 1, :]  # Extracting the 63rd variable (e.g., zonal wind)\n",
    "print(predictions_1d.shape, real_data_1d.shape)\n",
    "\n",
    "differences_in_tvd = []\n",
    "differences_in_ovl = []\n",
    "\n",
    "for feature in range(predictions_1d.shape[1]):\n",
    "    actual_hist, bin_edges = np.histogram(real_data_1d[:, feature], bins=50, density=True)\n",
    "    pred_hist, _ = np.histogram(predictions_1d[:, feature], bins=bin_edges, density=True)\n",
    "    tvd = total_variation_distance(pred_hist, actual_hist)\n",
    "    differences_in_tvd.append((tvd, feature))\n",
    "\n",
    "    # Calculate overlap coefficient (OVL) between the two histograms\n",
    "    ovl = np.sum(np.maximum(actual_hist,pred_hist)-np.minimum(actual_hist, pred_hist))\n",
    "    differences_in_ovl.append((ovl, feature))\n",
    "\n",
    "# Sort the differences in TVD\n",
    "sorted_differences_in_tvd = sorted(differences_in_tvd, key=lambda x: x[0])[::-1]\n",
    "sorted_differences_in_ovl = sorted(differences_in_ovl, key=lambda x: x[0])[::-1]\n",
    "\n",
    "# Print the sorted differences\n",
    "print(f\"Sorted Differences in TVD:{sorted_differences_in_tvd}\")\n",
    "print(f\"Sorted Differences in Overlap:{sorted_differences_in_ovl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b50532",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "folder = f\"testing_at_{timestamp}\"\n",
    "\n",
    "for i in range(0,30):\n",
    "    print(f\"Row {sorted_differences_in_ovl[i][1]}: TVD = {sorted_differences_in_ovl[i][0]:.6f}\")\n",
    "    data_real = real_data_1d[:, sorted_differences_in_ovl[i][1]]  # variable index 1 (e.g., zonal wind), level _\n",
    "    data_predictions = predictions_1d[:, sorted_differences_in_ovl[i][1]]  # variable index 0 (predictions), level _\n",
    "\n",
    "    print(f\"Shape of zonal_wind_data_real: {data_real.shape}\")\n",
    "    print(f\"Shape of zonal_wind_data_predictions: {data_predictions.shape}\")\n",
    "\n",
    "    # Plot the bimodal histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create histograms (overlaid)\n",
    "    sns.histplot(data_real, bins=50, kde=True, color='black', alpha=0.6, element='step', label='Real Data')\n",
    "    sns.histplot(data_predictions, bins=50, kde=True, color='red', alpha=0.6, element='step', label='Predictions')\n",
    "\n",
    "    # Customize plot labels and title\n",
    "    plt.title(f\"Distribution of Feature {sorted_differences_in_ovl[i][1]} For Real Data and Predictions\", fontsize=16)\n",
    "    plt.xlabel('Zonal Wind (m/s)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "    # Add vertical lines at means\n",
    "    plt.axvline(np.mean(data_real), color='black', linestyle='--', label=f'Real Mean: {np.mean(data_real):.2f}')\n",
    "    plt.axvline(np.mean(data_predictions), color='red', linestyle='--', label=f'Pred Mean: {np.mean(data_predictions):.2f}')\n",
    "\n",
    "    # Final plot settings\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(folder, \"bi_modal_distribution\")\n",
    "    save_path = os.path.join(save_path, \"bi_modal_distribution_plot\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
